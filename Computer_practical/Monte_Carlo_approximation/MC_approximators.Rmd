<!-- -------------------------------------------------------------------------------- -->

<!-- Copyright 2019 Georgios Karagiannis -->

<!-- georgios.karagiannis@durham.ac.uk -->
<!-- Assistant Professor -->
<!-- Department of Mathematical Sciences, Durham University, Durham,  UK  -->

<!-- This file is part of Bayesian_Statistics (MATH3341/4031 Bayesian Statistics III/IV) -->
<!-- which is the material of the course (MATH3341/4031 Bayesian Statistics III/IV) -->
<!-- taught by Georgios P. Katagiannis in the Department of Mathematical Sciences   -->
<!-- in the University of Durham  in Michaelmas term in 2019 -->

<!-- Bayesian_Statistics is free software: you can redistribute it and/or modify -->
<!-- it under the terms of the GNU General Public License as published by -->
<!-- the Free Software Foundation version 3 of the License. -->

<!-- Bayesian_Statistics is distributed in the hope that it will be useful, -->
<!-- but WITHOUT ANY WARRANTY; without even the implied warranty of -->
<!-- MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the -->
<!-- GNU General Public License for more details. -->

<!-- You should have received a copy of the GNU General Public License -->
<!-- along with Bayesian_Statistics  If not, see <http://www.gnu.org/licenses/>. -->

<!-- -------------------------------------------------------------------------------- -->

---
title: "Monte Carlo approximation"
subtitle: "An intoduction  for practical use in R"
author: "Georgios P. Karagiannis @ MATH3341/4031 Bayesian statistics III/IV (practical implementation)"
output:
  html_notebook: default
  word_document: default
  html_document:
    df_print: paged
  pdf_document: default
---
[Back to README](https://github.com/georgios-stats/Bayesian_Statistics/tree/master/ComputerPracticals#aim)

***Aim***

Students will be able to  
 
+ apply Monte Carlo approximation with R  

+ approximate integrals, expected values, modes, quantiles, etc... with R   

<!-- ***Reading material*** -->

<!-- + Nothing in particular, details will be learned in Term 2    -->

# Briefly ...   

***Monte Carlo approximation*** is a stochastic procedure for the evaluation of intractable quantities. It involves:         
1. properly generating a large random sample from a suitable distribution, and   
2. computing a statistic asymptotically consistent to the intractable quantity of interest.

This statistic is the Monte Carlo approximate of the intractable quantity. 

# Approximating expected values, and integrals...  

***Notation***

Assume a random variable $\theta\in\Theta$ following a distribution $\Pi(\cdot)$. 

Let $h(\cdot):\Theta\rightarrow\mathbb{R}^{d}$, $d\ge1$, be a measurable function. 

Assume that there is interest in approximating the expected value
\[
\text{E}_{\pi}(h(\theta)) 
= 
\int h(\theta) \text{d}\Pi(\theta)
=
\begin{cases}
\int h(\theta) \pi(\theta) \text{d}\theta, \ \text{cont.} \\
\\
\sum h(\theta) \pi(\theta) , \ \text{discr.} 
\end{cases}
\]


***Monte Carlo approximation procedure:***  

1. Draw a random sample 
\[
\theta^{(j)} \sim \Pi(\cdot), \  \text{for }j=1,...,N
\]

2. Compute 
\[
\bar{h}_{N} = \frac{1}{N}\sum_{j=1}^{N} h(\theta^{(j)})
\]

We say that **$\bar{h}_{N}$ is a Monte Carlo approximate of $\ \text{E}_{\pi}(h(\theta))$**, where   
\[
\text{E}_{\pi}(h(\theta))  \approx \bar{h}_{N} \ \text{ when $N$ is large enough.}
\] 
which can be justified by aw of Large Number (LLN) arguments: $\bar{h}_{N} \rightarrow \text{E}_{\pi}(h(\theta))  \ \text{ as } \ N\rightarrow\infty$. 

<!-- ***Justification***    -->

<!-- This can be justified by using arguments based on the Law of Large Number (LLN), which under specific assumtions (discussed in Term 2) implies that    -->
<!-- \[ -->
<!-- \bar{h}_{N} \rightarrow \text{E}_{\pi}(h(\theta))  \ \text{ as } \ N\rightarrow\infty. -->
<!-- \] -->


<!-- ***Monte Carlo approximations of popular quantities***  -->

<!-- Assume you have generated a random sample -->
<!-- \[ -->
<!-- \theta^{(j)} \sim \pi(\text{d}\theta), \  \text{for }j=1,...,N -->
<!-- \] -->

<!-- then    -->

<!-- + ***Mean*** -->
<!-- \[ -->
<!-- \text{E}_{\pi}(\theta)  \approx \frac{1}{N}\sum_{j=1}^{N} \theta^{(j)}. -->
<!-- \]  -->
<!--     + is approximated by the sample arithmetic mean. -->

<!-- + ***Variance*** -->
<!-- \begin{align*} -->
<!-- \text{Var}_{\pi}(\theta) -->
<!-- &= -->
<!-- \text{E}_{\pi}(\theta^{2})-(\text{E}_{\pi}(\theta))^{2} \\ -->
<!-- &\approx  -->
<!-- \frac{1}{N}\sum_{j=1}^{N} \left(\theta^{(j)}\right)^{2} - \left(\frac{1}{N}\sum_{j=1}^{N} \theta^{(j)}\right)^{2} -->
<!-- =s_\theta^2. -->
<!-- \end{align*} -->
<!--     + is approximated by the sample variance. -->

<!-- + ***Probability*** -->
<!-- \begin{align*} -->
<!-- \text{Pr}_{\pi}(\theta\in A) -->
<!-- &= -->
<!-- \text{E}_{\pi}(\text{1}(\theta\in A))\\ -->
<!-- &\approx  -->
<!-- \frac{1}{N}\sum_{j=1}^{N} \text{1}(\theta^{(j)}\in A). -->
<!-- \end{align*} -->
<!--     + is approximated by the sample frequency. -->

<!-- + ***Comulative Distribution Function (CDF)*** -->
<!-- \begin{align*} -->
<!-- \text{F}_{\pi}(\theta=c)=\text{Pr}_{\pi}(\theta\le c) -->
<!-- &= -->
<!-- \text{E}_{\pi}(\text{1}(\theta \le c)) \\ -->
<!-- &\approx  -->
<!-- \frac{1}{N}\sum_{j=1}^{N} \text{1}(\theta^{(j)} \le c). -->
<!-- \end{align*} -->
<!--     + is approximated by the empirical cumulative function. -->

<!-- + ***Probability Density Function (PDF)*** -->
<!-- \begin{align*} -->
<!-- \pi(\vartheta) -->
<!-- &= \lim_{\epsilon\rightarrow 0}\frac{\text{F}_{\pi}(\theta=\vartheta+\epsilon)-\text{F}_{\pi}(\theta=\vartheta-\epsilon)}{2\epsilon}\\ -->
<!-- &\approx\frac{1}{2\epsilon}\frac{1}{N}\sum_{j=1}^{N} \text{1}\left(\theta^{(j)}\in (\vartheta-\epsilon,\vartheta+\epsilon]\right) \  \text{for small }\epsilon -->
<!-- \end{align*}  -->
<!--     + is approximated by the histogram with bandwith $\epsilon$; i.e. `?hist`.    -->
<!--     + This approximation can be understood by plugging in the MC approximator of the CDF in the ratio and by chosing a 'small'  $\epsilon$ in the limit in the  exact  expresion.  -->

***Example***

Consider the integral
$$
I = \int_{0}^{\infty} x^{5} \exp(-3x)\text{d}x
$$
Compute a Monte Carlo approximate of integral $I$.

The Exact value is $I=$ `r (1/3)*prod(1:5)/(3^5)`.

***Solution***

It is
\begin{align*}
I 
&= \int_{0}^{\infty} x^{5} \exp(-3x)\text{d}x 
\  \  = \int x^{5} \frac{1}{3}3\exp(-3x)\text{1}(x\in(0,\infty))\text{d}x \\
&= \frac{1}{3}\int x^{5} \underset{=\text{d}\Pi_{\text{Exp}(3)}(x)}{\underbrace{3\exp(-3x)\text{1}(x\in(0,\infty))\text{d}x}}
\  \  = \frac{1}{3}\int x^{5}\text{d}\Pi_{\text{Exp}(3)}(x)
= \frac{1}{3}\text{E}_{\text{Exp}(3)} (x^{5}) \\
&\approx \frac{1}{3}\frac{1}{N}\sum_{j=1}^{N}\left(x^{(j)}\right)^{5}, \  \  \text{ where } \  \  x^{(j)}\sim  \text{Exp}(3)
\end{align*}

we compute the Monte Carlo approximate for $N=1000$.   


```{r}
x = rexp(n = 10^5, rate = 3)
I_mc = (1/3)*mean(x^5)
print(I_mc)
```


# Approximating other quantities...

Monte Carlo approximation can approximate quantities other than expected values, and the theoretical justification is out of the scope.

Let $\theta\in\Theta$ be a random variable with distribution $\Pi(\cdot)$.    

+ Assume there is a random sample generated: 
\[
\theta^{(j)} \sim \Pi( \cdot ), \  \text{for }j=1,...,N
\]
for $j=1,...,N$   

+ The characteristic of the distribution $\Pi(\cdot)$ such as population mode, quantiles , etc... can be approximated by their sample analogues such as sample mode, sample quantiles, etc...


<!-- Based on the MC approximation:       -->

<!-- + the population mode  $$M(\theta)=\arg\max_{\theta}\left(\pi(\theta)\right)$$ can be approximated by the sample mode computed from the sample  $\{\theta^{(1)},...,\theta^{(N)}\}$.   -->

<!-- $$ -->
<!-- \text{M}_{\pi}(h(\theta)) -->
<!-- \approx -->
<!-- \hat{\text{M}}\left(h(x^{(1)}),...,h(x^{(N)})\right) -->
<!-- $$ -->

<!-- + the population $\alpha$th quantile $$Q_{\alpha}(\theta)=\text{F}_{\pi}^{-1}(\alpha)$$ can be approximated by the empirical $\alpha$th quantile computed from the sample  $\{\theta^{(1)},...,\theta^{(N)}\}$.   -->


<!-- $$ -->
<!-- \text{Q}_{\pi}(h(\theta)) -->
<!-- \approx -->
<!-- \hat{\text{Q}}\left(h(x^{(1)}),...,h(x^{(N)})\right) -->
<!-- $$ -->


<!-- # Practical matters   -->

<!-- A this stage, several questions exist   -->

<!-- + What should be the sample size $N$ for the MC approximation to be reliable?.   -->

<!--     + you will learn thin in Term 2, but for now the larger the better. -->

<!-- + How can I draw the sample from the posterior distribution with PDF/PMF    -->
<!-- $$ -->
<!-- \pi(\theta|y_{1:n}) =\frac{f(y_{1:n}|\theta)\pi(\theta)}{\int f(y_{1:n}|\theta)\pi(\theta)\text{d}\theta} -->
<!-- $$ -->
<!-- or a predictive distribution with PDF/PMF  -->
<!-- $$ -->
<!-- f(y_{n+1}|y_{1:n}) =\int f(y_{n+1}|\theta) \pi(\text{d}\theta|y_{1:n}) -->
<!-- $$ -->
<!-- in cases that we cannot even calculate the integrals involved?   -->

<!--     + you can use the R function `stan{rstan}` that allows you to draw a sample.  -->



