<!-- -------------------------------------------------------------------------------- -->

<!-- Copyright 2019 Georgios Karagiannis -->

<!-- georgios.karagiannis@durham.ac.uk -->
<!-- Assistant Professor -->
<!-- Department of Mathematical Sciences, Durham University, Durham,  UK  -->

<!-- This file is part of Bayesian_Statistics (MATH3341/4031 Bayesian Statistics III/IV) -->
<!-- which is the material of the course (MATH3341/4031 Bayesian Statistics III/IV) -->
<!-- taught by Georgios P. Katagiannis in the Department of Mathematical Sciences   -->
<!-- in the University of Durham  in Michaelmas term in 2019 -->

<!-- Bayesian_Statistics is free software: you can redistribute it and/or modify -->
<!-- it under the terms of the GNU General Public License as published by -->
<!-- the Free Software Foundation version 3 of the License. -->

<!-- Bayesian_Statistics is distributed in the hope that it will be useful, -->
<!-- but WITHOUT ANY WARRANTY; without even the implied warranty of -->
<!-- MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the -->
<!-- GNU General Public License for more details. -->

<!-- You should have received a copy of the GNU General Public License -->
<!-- along with Bayesian_Statistics  If not, see <http://www.gnu.org/licenses/>. -->

<!-- -------------------------------------------------------------------------------- -->

---
title: "Normal model with conjugate priors"
subtitle: "Case study: Nissan Maxima data-set"
author: "Georgios P. Karagiannis @ MATH3341/4031 Bayesian statistics III/IV (practical implementation)"
output:
  html_notebook: 
    number_sections: true
  word_document: default
  pdf_document: default
  html_document:
    df_print: paged
    number_sections: true
---
[Back to README](https://github.com/georgios-stats/Bayesian_Statistics/tree/master/ComputerPracticals#aim)



```{r, results="hide"}
rm(list=ls())
```



---

***Aim***

Students will become able to:  

+ produce Monte Carlo approximations of posterior quantities required for Bayesian analysis with the RJAGS R package  

+ implement Bayesian posterior analysis in R with RJAGS package  

Students are not required to learn by heart any of the concepts discussed


---

***Reading material***

*The material about RJAGS package is not examinable material, but it is provided for the interested student. It contains references that students can follow if they want to further explore the concepts introdced.*

+ Lecture notes:  
    + the examples related to the Bernoulli model with conjugate prior 
+ References for *RJAGS*:  
    + [JAGS homepage](http://mcmc-jags.sourceforge.net)  
    + [JAGS R CRAN Repository](https://cran.r-project.org/web/packages/rjags/index.html)  
    + [JAGS Reference Manual](https://cran.r-project.org/web/packages/rjags/rjags.pdf)  
    + [JAGS user manual](https://sourceforge.net/projects/mcmc-jags/files/Manuals/4.x/jags_user_manual.pdf/download)  
+ Reference for *R*:  
    + [Cheat sheet with basic commands](https://www.rstudio.com/wp-content/uploads/2016/10/r-cheat-sheet-3.pdf)   
+ Reference of *rmarkdown* (optional):  
    + [R Markdown cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf)  
    + [R Markdown Reference Guide](http://442r58kc8ke1y38f62ssb208-wpengine.netdna-ssl.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf)  
    + [knitr options](https://yihui.name/knitr/options)
+ Reference for *Latex* (optional):  
    + [Latex Cheat Sheet](https://wch.github.io/latexsheet/latexsheet-a4.pdf)   

---

***New software***    

+ R package `rjags` functions:    
    + `jags.model{rjags}`  
    + `jags.samples{rjags}` 
    + `update{rjags}`

---

# Application: {-}

The applications below is 'reproduced' from [DASL](https://dasl.datadescription.com/datafile/nissan/), and 

+ De Veaux, R. D., Velleman, P. F., Bock, D. E., Vukov, A. M., Augustine, C. W., & Burkett, C. (2005). Stats: data and models. Boston: Pearson/Addison Wesley.  

Richard DeVeaux owned a Nissan Maxima for 8 years. Being a statistician, He recorded the car’s fuel efficiency (in mpg) each time he filled the tank. He wanted to know what fuel efficiency to expect as “ordinary” for his car. Knowing this, he was able to predict when he’d need to fill the tank again and to notice if the fuel efficiency suddenly got worse, which could be a sign of trouble.

**General scientific question:**

```{r}
# load the data
#mydata <- read.csv("./nissan.csv")
mydata <- read.csv("https://raw.githubusercontent.com/georgios-stats/Bayesian_Statistics/master/ComputerPracticals/Normal_model_with_conjugate_priors/nissan.csv")
#mydata$mpg <- rnorm(100, 20, 5)
# print data 
cat(mydata$mpg)

```

# Preliminary analysis  

## Task  

Examine graphically whether there is any substantial evidence that the above data-set is not generated from the Normal distribution.

Are there any substaintial evivence against Normality?

### ... your answer {-}

We can assume that the data-set generating prodedure can be represented by a Normal distribution family. This is based on the preliminary examination below.

```{r}
# Preliminary examination
## standardise me
z_obs <- (mydata$mpg - mean(mydata$mpg)) / sd(mydata$mpg) 
## plot me
par(mfrow=c(1,2))
qqnorm( z_obs,
  main = " qq-plot for Normality",
  ylab = "standardised mpg",
  xlab = "Normal quantiles"
)
hist(z_obs,
  main = "Empirical distr. of mpg",
  xlab = "standardised sample"
     )
```



---

# Model {-}

We specifiy the Normal distribution with uncertain mean and variance parameters $\mu$ and $\sigma^2$ as a parametric model. 

We assign a Normal-Inverse Gamma conjugate prior with fixed hyper-parameters $\mu_0=0.0$, $\lambda_0=1.0$, $a_0=1.0$, and $b_0=1.0$. 

The Bayesian hierarchical model under consideration is:  
\[
\begin{cases}
x_{i}|\mu,\sigma^{2} & \overset{\text{iid}}{\sim}\text{N}(\mu,\sigma^{2}),\,\,\forall i=1,...,n\\
\mu|\sigma^{2} & \sim\text{N}(\mu_{0},\frac{\sigma^{2}}{\lambda_{0}})\\
\sigma^{2} & \sim\text{G}(a,b)
\end{cases}
\]

for some fixed values of the hyper-parameters  $\lambda_{0}$, $a$, and $b$.

Here, we just set:   

+ $\mu_{0}=0.0$,  

+ $\lambda_{0}=1.0$,   

+ $a_0=0.01$,   

+ $b_0=0.01$  

# Task

1. Write the rjags program to draw a sample from the posterior distribution of the   parameters $\mu$, and $\sigma^2$     

2. Generate a sample of size $N=1000$ from the posterior distribution of the parameters.  

3. Draw trace plots of the samples desived

### ... your answer {-}

Load the library

```{r, results="hide"}
# Load rjags
library("rjags") 
```

Create an input script, for rjags, containing the Bayesian hierarchical model  
1. Write the Stan program to draw a sample from the posterior distribution of the   parameters $\mu$, and $\sigma^2$     

2. Generate a sample of size $N=1000$ from the posterior distribution of the parameters.  


```{r}
hierarhicalmodel <- "

  model {

  # this is related tot he sampling distribution
    
    tau_0 = lam_0 / sigma2
    
    for ( i in 1 : n ) {
      y[ i ] ~ dnorm( mu, tau )
    }
    
   # this is related to the prior distributions
    
    mu ~ dnorm( mu_0 , tau*lam_0 )
    sigma2 = 1 / tau
    tau ~ dgamma(a_0 , b_0)
    
  }

"
```


Create and list with inputs for rstan

```{r}
a_0 <- 0.01
b_0 <- 0.01
lam_0 <- 1.0
mu_0 <- 0.0
y_obs <- mydata$mpg ;
n_obs <- length( y_obs ) ;
data.bayes <- list(n = n_obs ,     # the number of the observations
                  y = y_obs ,     # the observations
                  a_0 = a_0,
                  b_0 = b_0,
                  lam_0 = lam_0,
                  mu_0 = mu_0
                  )
```


Create an input list, for jags, containing the data and fixed  parameters of the model 

```{r, results="hide"}
model.smpl <- jags.model( file = textConnection(hierarhicalmodel),
                          data = data.bayes)

```

Initialize the sampler with $N_{\text{adapt}}=1000$ iterations.   

+ This is a warming-up procedure (used as a black-box), where the sampler is automatically tuned and calibated before it starts generatign your samples.   

+ Regarding $N_{\text{adapt}}=1000$, the larger the better.

```{r}
adapt( object = model.smpl,
       n.iter = 1000 )
```

Generate a posterior sample of size $N=10000$.

Use  

+ `jags.samples{rjags}`  


We need to pay attention on two flaqs:  

+ the `n.iter`: the total size of the total sample sequence.   

+ the `variable.names`: it specifies the names of the random variables correspondign to the posterior samples I am interested in generating   

```{r, results="hide"}
N = 10000      # the size of the sample we ll gonna get
output = jags.samples( model= model.smpl,          # the model
                       variable.names= c("mu","sigma2"),    # names of variables to be sampled
                       n.iter = N                # size of sample
                       )
```

Check the names of the variables sampled  

+ use `names {base}`

```{r}
names(output)
```

Check the dimensions of each of the variables sampled  

+ use `dim {base}`

```{r}
dim( output$mu )
dim( output$sigma2 )
# the first dimension is the numbers of columns of the variable
# the second dimention is the size of the sample drawn
# the thirs dimention is the number of the sub-samples drawn (in our case it is just 1)
```

Copy the sample of each variable in a vector with a more friendly name...

```{r}
mu.smpl <-output$mu 
sigma2.smpl <-output$sigma2 
```


We plot the trace plots of the generated samples

```{r}
# draw the trace plots

par(mfrow=c(1,2))

mu.smpl <-output$mu 

plot(mu.smpl,
     type = "l",
     main = "Trace plot of mu",
     xlab = "iteration",
     ylab = "mu"
     )

sigma2.smpl <-output$sigma2 

plot(sigma2.smpl,
     type = "l",
     main = "Trace plot of sigma2",
     xlab = "iteration",
     ylab = "sigma2"
     )
```



# Task  

Produce and plot the exact joint PDF of $(\mu,\sigma^2)$ as a 2D plot, and as a 2D contour plot

To draw 3D plots like those required to plot the exact PDF you can use the functions `persp{graphics}`, or `contour{graphics}` as `persp(x, y , z)` or `contour(x, y, z)` , where  x, y	are locations of grid lines at which the values in z are measured; and z is a matrix containing the values to be plotted ;

***Usage***

    % To plot z=f(x,y) for a specific function f(.,.) at vectors x and y, then
    % for known x, y vectors of length 100
    
    z <- matrix(rep(NaN,100*100),100,100)
    for (i in 1:100)   
      for (j in 1:100) 
        z[i,j] <-f( x[i],z[j] )
    
    persp(x, y , z)
    
    % or
    
    contour(x, y, z)

***Hint-1***

The joint posterior distribution $(\mu,\sigma^{2})$, is:  
\[
\begin{cases}
\mu|x_{1:n},\sigma^2 
& \sim 
\text{N}(
\mu_{n},\frac{\sigma^{2}}{\lambda_{n}}
) \\
\sigma^2|x_{1:n} 
& \sim 
\text{IG}(
a_{n},b_{n}
)
\end{cases}
\]

where  
\begin{align*}
\mu_{n}=&
\frac{\lambda_{0}\mu_{n}+n\bar{x}}{\lambda_{0}+n} \\
\lambda_{n}=&
\lambda_{0}+n, \\
a_{n}=&
a_{0}+\frac{n}{2} \\
b_{n}=&
b_{0}+\frac{1}{2}ns^{2}+\frac{1}{2}(\lambda_{0}+n)^{-1}\lambda_{0}n(\mu_{0}-\bar{x})^{2} \\
\end{align*}

Hence the PDF is 
$$
\pi(\mu,\sigma^{2}|x_{1:n}) = 
\text{N}(\mu|\mu_{n},\frac{\sigma^{2}}{\lambda_{n}})
\text{IG}(\sigma^{2}|a_{n},b_{n})
$$

***Hint:*** 

Additional functions for PDFs and CDFs are given   

```{r}
# Inverse Gamma PDF  
invgamma_pdf <- function (x,a,b) {
  return(exp(a*log(b)-lgamma(a)-(a+1)*log(x)-b/x))
}
# Inverse Gamma CDF
invgamma_cdf <- function (x,a,b) {
  return(1.0-pgamma(1.0/x,shape=a,scale=1.0/b) )
}
```




### ... your answer {-}


```{r}
library(gplots)
```


```{r}

par(mfrow=c(1,2))

# EXACT

## compute the osterior parameters
a_n <- a_0 +0.5*n_obs
b_n <- b_0 +0.5*n_obs*var(y_obs) +0.5*lam_0*n_obs*(mu_0-mean(y_obs))^2/(lam_0+n_obs)
lam_n <- lam_0 + n_obs 
mu_n <- (lam_0*mu_0+n_obs*mean(y_obs)) / (lam_0+n_obs)

## plot
xmin = 21
xmax = 23
x_plot <- seq( from = xmin, 
               to = xmax, 
               length.out = 100)
ymin = 7
ymax = 19
y_plot <-seq( from = ymin, 
               to = ymax, 
               length.out = 100)
z_plot <- matrix(rep(NaN,100*100),100,100)
for (i in 1:100)
  for (j in 1:100)
    z_plot[i,j] <- dnorm( x_plot[i], mu_n, sqrt(y_plot[j]/lam_n) ) * invgamma_pdf(y_plot[j], a_n, b_n) ;

## plot
persp(x_plot, y_plot,  z_plot, 
      ticktype="detailed", theta=30, phi=30,
      expand=0.5, shade=0.5, col="cyan", ltheta=-30,
      xlab = 'mu',
      ylab='sigma2',
      zlab='pdf')
## plot
contour(x_plot, y_plot,  z_plot, 
      xlab = 'mu',
      ylab='sigma2')


```




# Task  

Produce and plot the MC approximation of the joint PDF of $(\mu,\sigma^2)$, as a 2D histogram, and as a 2D contour plot.

To draw histograms of 2D samples like required by the MC approximation of the PDF you can use the functions `hist2d{gplots}` and  `persp{graphics}`, or `contour{graphics}` as:  

    % known vectors x and y of length 100 having the 2D samples:  
    
    h2d <- hist2d(x,y,nbins=c(20,30),show=FALSE)
    
    persp( h2d$x, h2d$y, h2d$counts, ticktype="detailed", theta=30, phi=30,expand=0.5, shade=0.5, col="cyan", ltheta=-30)
    
    contour( h2d$x, h2d$y, h2d$counts, ticktype="detailed", theta=30, phi=30,expand=0.5, shade=0.5, col="cyan", ltheta=-30)


### ... your answer {-}


```{r}
library(gplots)
```


```{r}

mu.smpl <-output$mu 
sigma2.smpl <-output$sigma2 

# MC APPROXIMATION 

par(mfrow=c(1,2))

## extract the sample

h2d <-  hist2d(mu.smpl,
               sigma2.smpl, 
               nbins=c(20,30),
               show=FALSE)

## plot
persp(h2d$x, h2d$y, h2d$counts, 
      ticktype="detailed", theta=30, phi=30,
      expand=0.5, shade=0.5, col="cyan", ltheta=-30,
      xlab = 'mu',
      ylab='sigma2',
      zlab='pdf')

## plot
contour(h2d$x, h2d$y, h2d$counts, 
      xlab = 'mu',
      ylab='sigma2')


```



# Task  

Produce and plot the Exact   marginal PDF   of $\mu$. 

Produce and plotthe MC approximation of the marginal PDF   of $\mu$. 

You can use the functions  `hist {graphics}`  and  `lines {graphics}` or  `plots {graphics}`

***Hint***

The marginal distribution of $\mu$, is:  
  \[
    \mu|x_{1:n}\sim\text{St}_{k}(\mu_{n},\frac{b_{n}}{\lambda_{n}a_{n}},2a_{n})
    \]

where  
\begin{align*}
\mu_{n}=&
  \frac{\lambda_{0}\mu_{n}+n\bar{x}}{\lambda_{0}+n} \\
\lambda_{n}=&
  \lambda_{0}+n, \\
a_{n}=&
  a_{0}+\frac{n}{2} \\
b_{n}=&
  b_{0}+\frac{1}{2}ns^{2}+\frac{1}{2}(\lambda_{0}+n)^{-1}\lambda_{0}n(\mu_{0}-\bar{x})^{2} \\
\end{align*}


***Hint:*** 

Additional PDFs and CDFs are given   

```{r}
# Student T PDF
studentT_pdf <- function(x,m,s,v) {
  return(dt((x-m)/sqrt(s),df=v)/sqrt(s))
}
# Student T CDF
studentT_cdf <- function(x,m,s,v) {
  return(pt((x-m)/sqrt(s),df=v))
}
```


### ... your answer {-}


For the marginal posterior PDF of $\mu$

```{r}

# Draw the histogram as the MC approximate of the PDF

mu.smpl <-output$mu 

z <- mu.smpl
hist(z,
     probability = TRUE,
     main = "Post PDF of mu",
     xlab = "mu",
     ylab = "PDF")


# Draw the exact PDF

## Posterior parameters
a_n <- a_0 +0.5*n_obs
b_n <- b_0 +0.5*n_obs*var(y_obs) +0.5*lam_0*n_obs*(mu_0-mean(y_obs))^2/(lam_0+n_obs)
lam_n <- lam_0 + n_obs 
mu_n <- (lam_0*mu_0+n_obs*mean(y_obs)) / (lam_0+n_obs)

x_plot <- seq( from = min(z), to = max(z), length.out = 100)
y_plot <- studentT_pdf(x_plot , mu_n , b_n/(a_n*lam_n), 2*a_n)
lines(x_plot, 
      y_plot,
      type = 'l',
      col = 'red')
legend("topright",
       legend=c("MC approx.", "Exact"),
       lty = c(1,1),
       col=c("black", "red"))
```

For the marginal posterior CDF of $\mu$

```{r}

# Draw the histogram as the MC approximate of the CDF

## extract the sample, and copy it to smple
mu.smpl <-output$mu

z <- mu.smpl
x_plot <- seq( from = mean(z)-4*sd(z), to = mean(z)+4*sd(z), length.out = 100)
y_plot <- rep(NaN, 100)
for (i in 1:100) y_plot[i] <- mean(z<=x_plot[i])
plot(x_plot,
     y_plot,
     type = "l",
     main = "CDF of mu",
     xlab = "mu",
     ylab = "CDF")

#  Draw the exact CDF

## Posterior parameters
a_n <- a_0 +0.5*n_obs
b_n <- b_0 +0.5*n_obs*var(y_obs) +0.5*lam_0*n_obs*(mu_0-mean(y_obs))^2/(lam_0+n_obs)
lam_n <- lam_0 + n_obs 
mu_n <- (lam_0*mu_0+n_obs*mean(y_obs)) / (lam_0+n_obs)
x_plot <- seq( from = min(z), to = max(z), length.out = 100)
y_plot <- studentT_cdf(x_plot , mu_n , b_n/(a_n*lam_n), 2*a_n)
lines(x_plot, 
      y_plot,
      type = 'l',
      col = 'red')

legend("topleft",
       legend=c("MC approx.", "Exact"),
       lty = c(1,1),
       col=c("black", "red"))
```


# Task  

Produce and plot the Exact   marginal PDF of $\sigma^2$. 

Produce and plot  the MC approximation of the marginal PDF   of $\sigma^2$. 


You can use the functions  `hist {graphics}`  and  `lines {graphics}` or  `plot {graphics}`

***Hint***

The marginal distributions of $\sigma^{2}$, as:
  \[
    \sigma^2|x_{1:n} 
    \sim 
    \text{IG}(\sigma^{2}|a_{n},b_{n})
    \]

where  
\begin{align*}
\mu_{n}=&
  \frac{\lambda_{0}\mu_{n}+n\bar{x}}{\lambda_{0}+n} \\
\lambda_{n}=&
  \lambda_{0}+n, \\
a_{n}=&
  a_{0}+\frac{n}{2} \\
b_{n}=&
  b_{0}+\frac{1}{2}ns^{2}+\frac{1}{2}(\lambda_{0}+n)^{-1}\lambda_{0}n(\mu_{0}-\bar{x})^{2} \\
\end{align*}


***Hint:*** 

Additional PDFs and CDFs are given   

```{r}
# Inverse Gamma PDF  
invgamma_pdf <- function (x,a,b) {
  return(exp(a*log(b)-lgamma(a)-(a+1)*log(x)-b/x))
}
# Inverse Gamma CDF
invgamma_cdf <- function (x,a,b) {
  return(1.0-pgamma(1.0/x,shape=a,scale=1.0/b) )
}
```



### ... your answer {-}


For the marginal posterior PDF of $\sigma^2$

```{r}

# Draw the histogram as the MC approximate of the PDF

## extract the sample, and copy it to smple
sigma2.smpl <-output$sigma2 

z <- sigma2.smpl
hist(z,
     probability = TRUE,
     main = "Post PDF of sigma2",
     xlab = "sigma2",
     ylab = "PDF")

# Draw the exact PDF

## Posterior parameters
a_n <- a_0 +0.5*n_obs
b_n <- b_0 +0.5*n_obs*var(y_obs) +0.5*lam_0*n_obs*(mu_0-mean(y_obs))^2/(lam_0+n_obs)
lam_n <- lam_0 + n_obs 
mu_n <- (lam_0*mu_0+n_obs*mean(y_obs)) / (lam_0+n_obs)

x_plot <- seq( from = min(z), to = max(z), length.out = 100)
y_plot <- invgamma_pdf(x_plot , a_n, b_n)
lines(x_plot, 
      y_plot,
      type = 'l',
      col = 'red')
legend("topright",
       legend=c("MC approx.", "Exact"),
       lty = c(1,1),
       col=c("black", "red"))
```

For the marginal posterior CDF of $\sigma^2$

```{r}

# Draw the histogram as the MC approximate of the CDF

## extract the sample, and copy it to smple
sigma2.smpl <-output$sigma2 

z <- sigma2.smpl
x_plot <- seq( from = mean(z)-4*sd(z), to = mean(z)+4*sd(z), length.out = 100)
y_plot <- rep(NaN, 100)
for (i in 1:100) y_plot[i] <- mean(z<=x_plot[i])
plot(x_plot,
     y_plot,
     type = "l",
     main = "CDF of sigma2",
     xlab = "sigma2",
     ylab = "CDF")

# Draw the exact CDF

## Posterior parameters
a_n <- a_0 +0.5*n_obs
b_n <- b_0 +0.5*n_obs*var(y_obs) +0.5*lam_0*n_obs*(mu_0-mean(y_obs))^2/(lam_0+n_obs)
lam_n <- lam_0 + n_obs 
mu_n <- (lam_0*mu_0+n_obs*mean(y_obs)) / (lam_0+n_obs)

x_plot <- seq( from = min(z), to = max(z), length.out = 100)
y_plot <- invgamma_cdf(x_plot , a_n, b_n)
lines(x_plot, 
      y_plot,
      type = 'l',
      col = 'red')
legend("topleft",
       legend=c("MC approx.", "Exact"),
       lty = c(1,1),
       col=c("black", "red"))
```


# Task 

+ Compute the MC approximate of the posterior mean of $\mu$, and $\sigma^2$:  
$$
\text{E}_\pi (\mu|y_{1:n})
\approx
\frac{1}{N}\sum_{j=1}^{N}\mu^{(j)}
$$  
and   
$$
\text{E}_\pi (\sigma^2|y_{1:n})
\approx
\frac{1}{N}\sum_{j=1}^{N} \left(\sigma^{(j)}\right)^2
$$  

+ Compute their exact values which are   
$$\text{E}_\pi (\mu|y_{1:n}) = \mu_n$$  
and  
$$\text{E}_\pi (\sigma^2|y_{1:n}) = \frac{b_n}{a_n-1}$$  



### ... your answer {-}


The Monte Carlo approximation for $\text{E}_\pi (\mu|y_{1:n})$ is  

```{r}
mu.smpl <-output$mu 
mc.approx <- mean(mu.smpl)
mc.approx
```

The exact value for $\text{E}_\pi (\mu|y_{1:n})=\mu_n$ is  

```{r}
# Posterior parameters
mu_n <- (lam_0*mu_0+n_obs*mean(y_obs)) / (lam_0+n_obs)
mu_n 
```

The Monte Carlo approximation for $\text{E}_\pi (\sigma^2|y_{1:n})$ is  

```{r}
sigma2.smpl <-output$sigma2 
mc.approx <- mean(sigma2.smpl)
mc.approx
```

The exact value for $\text{E}_\pi (\sigma^2|y_{1:n})= \frac{b_n}{a_n-1}$ is  

```{r}
# Posterior parameters
a_n <- a_0 +0.5*n_obs
b_n <- b_0 +0.5*n_obs*var(y_obs) +0.5*lam_0*n_obs*(mu_0-mean(y_obs))^2/(lam_0+n_obs)
exct <- b_n/(a_n-1)
exct
```

# Task 

+ Compute the MC approximate of the posterior prabability that the mean  fuel efficiency (in mpg)  of car is greater or equal to  $22.5$  mpg.  
\begin{align}
\text{Pr}_\pi (\mu\ge 22.5|y_{1:n}) 
&=
1-\text{Pr}_\pi (\mu< 22.5|y_{1:n}) \\
&=
1-\text{E}_\pi (\text{1}(\mu< 22.5)|y_{1:n}) \\
&\approx
1-\frac{1}{N}\sum_{j=1}^{N}\text{1}(\mu^{(j)} < 22.5) 
\end{align}   

+ Compute the exact value which is
\begin{align}
\text{Pr}_\pi (\mu\ge 22.5|y_{1:n}) =& 1-\text{Pr}_{\text{St}_{k}(\mu_{n},\frac{b_{n}}{\lambda_{n}a_{n}},2a_{n})}(\mu\le 22.5|y_{1:n}) \\
=&
1-\int_{-\infty}^{22.5}\text{St}_{k}(\mu|\mu_{n},\frac{b_{n}}{\lambda_{n}a_{n}},2a_{n})\text{d}\mu 
\end{align}   

***Hint***

Functions for the PDFs and CDF are given

```{r}
# Student T PDF
studentT_pdf <- function(x,m,s,v) {
  return(dt((x-m)/sqrt(s),df=v)/sqrt(s))
}
# Student T CDF
studentT_cdf <- function(x,m,s,v) {
  return(pt((x-m)/sqrt(s),df=v))
}
```


### ... your answer {-}


The MC approximate is  


```{r}
## extract the sample, and copy it to smple
mu.smpl <-output$mu 
Pr.mu.mc <- mean( mu.smpl>=22.5 )
Pr.mu.mc
```

and its exact value is  ...

```{r}
## Posterior parameters
a_n <- a_0 +0.5*n_obs
b_n <- b_0 +0.5*n_obs*var(y_obs) +0.5*lam_0*n_obs*(mu_0-mean(y_obs))^2/(lam_0+n_obs)
lam_n <- lam_0 + n_obs 
mu_n <- (lam_0*mu_0+n_obs*mean(y_obs)) / (lam_0+n_obs)
## The Exact value is
Pr.mu.ex <- 1-studentT_cdf(22.5, mu_n, b_n/(lam_n*a_n), 2*a_n)
Pr.mu.ex

```

The MC approximation is good!

---

# Task

Compute the exact $95\%$ equal tail posterior credible interval of $\mu$.

***Hint***

The marginal distribution of $\mu$, is:  
  \[
    \mu|x_{1:n}\sim\text{St}_{k}(\mu_{n},\frac{b_{n}}{\lambda_{n}a_{n}},2a_{n})
    \]

where  
\begin{align*}
\mu_{n}=&
  \frac{\lambda_{0}\mu_{n}+n\bar{x}}{\lambda_{0}+n} \\
\lambda_{n}=&
  \lambda_{0}+n, \\
a_{n}=&
  a_{0}+\frac{n}{2} \\
b_{n}=&
  b_{0}+\frac{1}{2}ns^{2}+\frac{1}{2}(\lambda_{0}+n)^{-1}\lambda_{0}n(\mu_{0}-\bar{x})^{2} \\
\end{align*}

Compute the MC approximation of the $95\%$ equal tail posterior credible interval of $\mu$.

***Hint:***

Additional PDFs and CDFs are given   

```{r}
# Student T INVERSE CDF
studentT_inv <- function(prob,m,s,v) {
  q = qt( prob, df=v )
  return( m+sqrt(s*v/(v-2))*q )
}
```



### ... your answer {-}


The exact CI is 
```{r}
# Posterior parameters
a_n <- a_0 +0.5*n_obs
b_n <- b_0 +0.5*n_obs*var(y_obs) +0.5*lam_0*n_obs*(mu_0-mean(y_obs))^2/(lam_0+n_obs)
lam_n <- lam_0 + n_obs 
mu_n <- (lam_0*mu_0+n_obs*mean(y_obs)) / (lam_0+n_obs)
# CI
studentT_inv(c(0.025,0.975), mu_n , b_n/lam_n/a_n, 2*a_n )
```



The MC approximation of the  CI is 
```{r}
# extract the sample, and copy it to smple
mu.smpl <-output$mu 
quantile(mu.smpl, probs =c(0.025,0.975))
```

---


# Task

Compute the exact $95\%$ equal tail posterior credible interval of $\mu$.

***Hint***

The marginal distribution of $\mu$, is:  
  \[
    \mu|x_{1:n}\sim\text{St}_{k}(\mu_{n},\frac{b_{n}}{\lambda_{n}a_{n}},2a_{n})
    \]

where  
\begin{align*}
\mu_{n}=&
  \frac{\lambda_{0}\mu_{n}+n\bar{x}}{\lambda_{0}+n} \\
\lambda_{n}=&
  \lambda_{0}+n, \\
a_{n}=&
  a_{0}+\frac{n}{2} \\
b_{n}=&
  b_{0}+\frac{1}{2}ns^{2}+\frac{1}{2}(\lambda_{0}+n)^{-1}\lambda_{0}n(\mu_{0}-\bar{x})^{2} \\
\end{align*}

Compute the MC approximation of the $95\%$ equal tail posterior credible interval of $\mu$.

***Hint:***

Additional PDFs and CDFs are given   

```{r}
# Student T INVERSE CDF
invgamma_inv <- function (prob, a, b){
    return(qgamma(1 - prob, a, b)^(-1))
}
```



### ... your answer {-}


The exact CI is 
```{r}
# Posterior parameters
a_n <- a_0 +0.5*n_obs
b_n <- b_0 +0.5*n_obs*var(y_obs) +0.5*lam_0*n_obs*(mu_0-mean(y_obs))^2/(lam_0+n_obs)
# CI
invgamma_inv(c(0.025,0.975), a_n , b_n )
```



The MC approximation of the  CI is 
```{r}
# extract the sample, and copy it to smple
sigma2.smpl <-output$sigma2 
quantile(sigma2.smpl, probs =c(0.025,0.975))
```

---

# Task


+ Compute and plot the MC approximate of the predictive PDF of the next outcome $y_{n+1}$
\begin{align*}
f(y_{n+1}|y_{1:n}) 
=& \int f(y_{n+1}|\mu,\sigma^2) \pi(\mu,\sigma^2|y_{1:n}) \text{d}\mu\text{d}\sigma^2 \\
=& \text{E}_\pi (f(y_{n+1}|\mu,\sigma^2)|y_{1:n}) \\
=& \text{E}_\pi (\text{N}(y_{n+1}|\mu,\sigma^2)|y_{1:n}) \\
\approx &
\frac{1}{N}\sum_{j=1}^{N} \text{N}\left(y_{n+1}|\mu^{(j)},\left(\sigma^{(j)}\right)^2\right)
\end{align*} 
for $y_{n+1}\in(5,40)$ where $f(y_{n+1}|\mu,\sigma^2)$ is a Normal PDF with mean $\mu$ and variance $\sigma^2$.

+ Compute and plot the exact predictive PDF of the next outcome $y_{n+1}$, which is the PDF of 
\[
y_{n+1}|y_{1:n}\sim\text{St}(\mu_n,\frac{\lambda_n b_n}{(\lambda_n+1)a_n},2a_n)
\] 
where
\begin{align*}
\mu_{n}=&
\frac{\lambda_{0}\mu_{n}+n\bar{x}}{\lambda_{0}+n} \\
\lambda_{n}=&
\lambda_{0}+n, \\
a_{n}=&
a_{0}+\frac{n}{2} \\
b_{n}=&
b_{0}+\frac{1}{2}ns^{2}+\frac{1}{2}(\lambda_{0}+n)^{-1}\lambda_{0}n(\mu_{0}-\bar{x})^{2} \\
\end{align*}
 for $y_{n+1}\in(5,40)$.

***Hint***

Additional PDF and CDF functions are given

```{r}
# Student T PDF
studentT_pdf <- function(x,m,s,v) {
  return(dt((x-m)/sqrt(s),df=v)/sqrt(s))
}
# Student T CDF
studentT_cdf <- function(x,m,s,v) {
  return(pt((x-m)/sqrt(s),df=v))
}
```



### ... your answer {-}



The MC approximation is such as 
\begin{align*}
f(y_{n+1}|y_{1:n}) 
=& \int f(y_{n+1}|\mu,\sigma^2) \pi(\mu,\sigma^2|y_{1:n}) \text{d}\mu\text{d}\sigma^2 \\
=& \text{E}_\pi (f(y_{n+1}|\mu,\sigma^2)|y_{1:n}) \\
=& \text{E}_\pi (\text{N}(y_{n+1}|\mu,\sigma^2)|y_{1:n}) \\
\approx& \frac{1}{N} \sum_{i=1}^{N} \text{N}(y_{n+1}|\mu^{(i)},(\sigma^{(i)})^2)
\end{align*}


```{r}

# Draw the MC approximation plot

## extract the sample, and copy it to smple
mu.smpl <-output$mu 
sigma2.smpl <-output$sigma2 

x_plot <- seq( from = min(
                        mean(mu.smpl)-5*mean(sqrt(sigma2.smpl))
                        ), 
               to = max(
                        mean(mu.smpl)+5*mean(sqrt(sigma2.smpl))
                        ), 
               length.out = 100)
y_plot <- rep(NaN,100)
for (i in 1:100) 
  y_plot[i] <- mean( dnorm( x_plot[i], 
                         mu.smpl,
                         sqrt(sigma2.smpl) 
                         )  )
plot(x_plot,
      y_plot,
      type = 'l',
     main = "Post PDF of y_new",
     xlab = "y_new",
     ylab = "PDF")

# Draw the exact now

## Posterior parameters
a_n <- a_0 +0.5*n_obs
b_n <- b_0 +0.5*n_obs*var(y_obs) +0.5*lam_0*n_obs*(mu_0-mean(y_obs))^2/(lam_0+n_obs)
lam_n <- lam_0 + n_obs
mu_n <- (lam_0*mu_0+n_obs*mean(y_obs)) / (lam_0+n_obs)
##  draw the plot
### I m using the x_plot fromt he previous plot

y_plot <- studentT_pdf(x_plot , mu_n , (lam_n+1)*b_n/((lam_n)*a_n), 2*a_n)
lines(x_plot,
      y_plot,
      type = 'l',
      col = 'red')
legend("topright",
       legend=c("MC approx.", "Exact"),
       lty = c(1,1),
       col=c("black", "red"))

```


---



# Task

+ Compute   the MC approximate of the predictive expected value of the next outcome $y_{n+1}$
\begin{align*}
\text{E}_f(y_{n+1}|y_{1:n})
=& \int y_{n+1} f(y_{n+1}|y_{1:n}) \text{d} y_{n+1} \\
=& \int y_{n+1} \left( \int f(y_{n+1}|\mu,\sigma^2) \pi(\mu,\sigma^2|y_{1:n}) \text{d}\mu\text{d}\sigma^2 \right) \text{d} y_{n+1} \\
=& \int  \left( \int y_{n+1} f(y_{n+1}|\mu,\sigma^2) \text{d} y_{n+1} \right)  \pi(\mu,\sigma^2|y_{1:n})  \text{d}\mu\text{d}\sigma^2  \\
=& \text{E}_\pi ( \text{E}_f (y_{n+1}|\mu,\sigma^2)  |y_{1:n}) \\
=& \text{E}_\pi (\mu |y_{1:n}) \\
\approx& \frac{1}{N} \sum_{i=1}^{N}  \mu^{(i)}
\end{align*}

+ Compute   the exact  predictive expected value of the next outcome $y_{n+1}$ which is  
\begin{align*}
\text{E}_f(y_{n+1}|y_{1:n})
=& \text{E}_\pi ( \text{E}_f (y_{n+1}|\mu,\sigma^2)  |y_{1:n}) \\
=& \text{E}_\pi (\mu |y_{1:n}) \\
=& \mu_n
\end{align*}
 


### ... your answer {-}



Regarding the MC approximate value:  
```{r}
mu.smpl <-output$mu 
mu.mc <- mean(mu.smpl)
mu.mc
```


Regarding the exact value:  
```{r}
# Posterior parameters
a_n <- a_0 +0.5*n_obs
b_n <- b_0 +0.5*n_obs*var(y_obs) +0.5*lam_0*n_obs*(mu_0-mean(y_obs))^2/(lam_0+n_obs)
lam_n <- lam_0 + n_obs
mu_n <- (lam_0*mu_0+n_obs*mean(y_obs)) / (lam_0+n_obs)
mu_n
```





# Task

+ Compute   the MC approximate of the predictive variance of the next outcome $y_{n+1}$
\begin{align*}
\text{Var}_f(y_{n+1}|y_{1:n})
&=
\text{E}_{\pi} \left( \text{Var}_{f} \left( y_{n+1}|\mu,\sigma^{2}\right) \right)
+\text{Var}_{\pi} \left( \text{E}_{f} \left( y_{n+1}|\mu,\sigma^{2}\right) \right) \\
&=
\text{E}_{\pi} \left( \sigma^{2}|y_{1:n} \right)
+\text{Var}_{\pi} \left(\mu |y_{1:n} \right) \\
&\approx
\frac{1}{N}\sum_{j=1}^{N} \left( \sigma^{(j)} \right)^{2}
+\frac{1}{N}\sum_{j=1}^{N} \left( \left( \mu^{(j)} \right)^{2}\right)
-\left( \frac{1}{N}\sum_{j=1}^{N} \mu^{(j)} \right)^{2} 
\end{align*}

+ Compute   the exact  predictive expected value of the next outcome $y_{n+1}$ which is  
\begin{align*}
\text{Var}_f(y_{n+1}|y_{1:n})
&=
\frac{\lambda_n b_n}{(\lambda_n+1)(a_n-1)}
\end{align*} 
where
\begin{align*}
\mu_{n}=&
\frac{\lambda_{0}\mu_{n}+n\bar{x}}{\lambda_{0}+n} \\
\lambda_{n}=&
\lambda_{0}+n, \\
a_{n}=&
a_{0}+\frac{n}{2} \\
b_{n}=&
b_{0}+\frac{1}{2}ns^{2}+\frac{1}{2}(\lambda_{0}+n)^{-1}\lambda_{0}n(\mu_{0}-\bar{x})^{2} \\
\end{align*}
because 
\[
y_{n+1}|y_{1:n}\sim\text{St}(\mu_n,\frac{\lambda_n b_n}{(\lambda_n+1)a_n},2a_n)
\] 




### ... your answer {-}



Regarding the MC approximate value:  
```{r}
## extract the sample, and copy it to smple
mu.smpl <-output$mu 
sigma2.smpl <-output$sigma2 
# compute the MC approximation
y.var.mc <- mean(sigma2.smpl) + mean(mu.smpl^2) - mean(mu.smpl)^2
y.var.mc
```


Regarding the exact value:  
```{r}
# Posterior parameters
a_n <- a_0 +0.5*n_obs
b_n <- b_0 +0.5*n_obs*var(y_obs) +0.5*lam_0*n_obs*(mu_0-mean(y_obs))^2/(lam_0+n_obs)
lam_n <- lam_0 + n_obs
y.var <-  lam_n/(lam_n+1)*b_n/(a_n)
y.var
```










