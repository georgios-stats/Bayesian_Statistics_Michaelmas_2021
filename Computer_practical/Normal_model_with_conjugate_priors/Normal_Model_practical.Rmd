<!-- -------------------------------------------------------------------------------- -->

<!-- Copyright 2019 Georgios Karagiannis -->

<!-- georgios.karagiannis@durham.ac.uk -->
<!-- Assistant Professor -->
<!-- Department of Mathematical Sciences, Durham University, Durham,  UK  -->

<!-- This file is part of Bayesian_Statistics (MATH3341/4031 Bayesian Statistics III/IV) -->
<!-- which is the material of the course (MATH3341/4031 Bayesian Statistics III/IV) -->
<!-- taught by Georgios P. Katagiannis in the Department of Mathematical Sciences   -->
<!-- in the University of Durham  in Michaelmas term in 2019 -->

<!-- Bayesian_Statistics is free software: you can redistribute it and/or modify -->
<!-- it under the terms of the GNU General Public License as published by -->
<!-- the Free Software Foundation version 3 of the License. -->

<!-- Bayesian_Statistics is distributed in the hope that it will be useful, -->
<!-- but WITHOUT ANY WARRANTY; without even the implied warranty of -->
<!-- MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the -->
<!-- GNU General Public License for more details. -->

<!-- You should have received a copy of the GNU General Public License -->
<!-- along with Bayesian_Statistics  If not, see <http://www.gnu.org/licenses/>. -->

<!-- -------------------------------------------------------------------------------- -->

---
title: "Normal model with conjugate priors"
subtitle: "Case study: Nissan Maxima data-set"
author: "Georgios P. Karagiannis @ MATH3341/4031 Bayesian statistics III/IV (practical implementation)"
output:
  html_notebook: 
    number_sections: true
  word_document: default
  html_document:
    df_print: paged
    number_sections: true
  pdf_document: default
---
[Back to README](https://github.com/georgios-stats/Bayesian_Statistics/tree/master/ComputerPracticals#aim)



```{r, results="hide"}
rm(list=ls())
```



---

***Aim***

Students will become able to:  

+ produce Monte Carlo approximations of posterior quantities required for Bayesian analysis with the RJAGS R package  

+ implement Bayesian posterior analysis in R with RJAGS package  

Students are not required to learn by heart any of the concepts discussed


---

***Reading material***

*The material about RJAGS package is not examinable material, but it is provided for the interested student. It contains references that students can follow if they want to further explore the concepts introdced.*

+ Lecture notes:  
    + the examples related to the Bernoulli model with conjugate prior 
+ References for *RJAGS*:  
    + [JAGS homepage](http://mcmc-jags.sourceforge.net)  
    + [JAGS R CRAN Repository](https://cran.r-project.org/web/packages/rjags/index.html)  
    + [JAGS Reference Manual](https://cran.r-project.org/web/packages/rjags/rjags.pdf)  
    + [JAGS user manual](https://sourceforge.net/projects/mcmc-jags/files/Manuals/4.x/jags_user_manual.pdf/download)  
+ Reference for *R*:  
    + [Cheat sheet with basic commands](https://www.rstudio.com/wp-content/uploads/2016/10/r-cheat-sheet-3.pdf)   
+ Reference of *rmarkdown* (optional):  
    + [R Markdown cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf)  
    + [R Markdown Reference Guide](http://442r58kc8ke1y38f62ssb208-wpengine.netdna-ssl.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf)  
    + [knitr options](https://yihui.name/knitr/options)
+ Reference for *Latex* (optional):  
    + [Latex Cheat Sheet](https://wch.github.io/latexsheet/latexsheet-a4.pdf)   

---

***New software***    

+ R package `rjags` functions:    
    + `jags.model{rjags}`  
    + `jags.samples{rjags}` 
    + `update{rjags}`

---

# Application: {-}

The applications below is 'reproduced' from [DASL](https://dasl.datadescription.com/datafile/nissan/), and 

+ De Veaux, R. D., Velleman, P. F., Bock, D. E., Vukov, A. M., Augustine, C. W., & Burkett, C. (2005). Stats: data and models. Boston: Pearson/Addison Wesley.  

Richard DeVeaux owned a Nissan Maxima for 8 years. Being a statistician, He recorded the car’s fuel efficiency (in mpg) each time he filled the tank. He wanted to know what fuel efficiency to expect as “ordinary” for his car. Knowing this, he was able to predict when he’d need to fill the tank again and to notice if the fuel efficiency suddenly got worse, which could be a sign of trouble.

**General scientific question:**

```{r}
# load the data
#mydata <- read.csv("./nissan.csv")
mydata <- read.csv("https://raw.githubusercontent.com/georgios-stats/Bayesian_Statistics/master/ComputerPracticals/Normal_model_with_conjugate_priors/nissan.csv")
# print data 
cat(mydata$mpg)

```

# Preliminary analysis  

## Task  

Examine graphically whether there is any substantial evidence that the above data-set is not generated from the Normal distribution.

Are there any substaintial evivence against Normality?



### ... your answer {-}

Below try to address the task by creating R chunks, and running them, and adding comments tot he document.

```{r}
# write your R code


```



---

# Model {-}

We specifiy the Normal distribution with uncertain mean and variance parameters $\mu$ and $\sigma^2$ as a parametric model. 

We assign a Normal-Inverse Gamma conjugate prior with fixed hyper-parameters $\mu_0=0.0$, $\lambda_0=1.0$, $a_0=1.0$, and $b_0=1.0$. 

The Bayesian hierarchical model under consideration is:  
\[
\begin{cases}
x_{i}|\mu,\sigma^{2} & \overset{\text{iid}}{\sim}\text{N}(\mu,\sigma^{2}),\,\,\forall i=1,...,n\\
\mu|\sigma^{2} & \sim\text{N}(\mu_{0},\frac{\sigma^{2}}{\lambda_{0}})\\
\sigma^{2} & \sim\text{G}(a,b)
\end{cases}
\]

for some fixed values of the hyper-parameters  $\lambda_{0}$, $a$, and $b$.

Here, we just set:   

+ $\mu_{0}=0.0$,  

+ $\lambda_{0}=1.0$,   

+ $a_0=0.01$,   

+ $b_0=0.01$  

# Task

1. Write the rjags program to draw a sample from the posterior distribution of the   parameters $\mu$, and $\sigma^2$     

2. Generate a sample of size $N=1000$ from the posterior distribution of the parameters.  

3. Draw trace plots of the samples desived


### ... your answer {-}

Below try to address the task by creating R chunks, and running them, and adding comments tot he document.

```{r}
# write your R code


```


# Task  

Produce and plot the exact joint PDF of $(\mu,\sigma^2)$ as a 2D plot, and as a 2D contour plot

To draw 3D plots like those required to plot the exact PDF you can use the functions `persp{graphics}`, or `contour{graphics}` as `persp(x, y , z)` or `contour(x, y, z)` , where  x, y	are locations of grid lines at which the values in z are measured; and z is a matrix containing the values to be plotted ;

***Usage***

    % To plot z=f(x,y) for a specific function f(.,.) at vectors x and y, then
    % for known x, y vectors of length 100
    
    z <- matrix(rep(NaN,100*100),100,100)
    for (i in 1:100)   
      for (j in 1:100) 
        z[i,j] <-f( x[i],z[j] )
    
    persp(x, y , z)
    
    % or
    
    contour(x, y, z)

***Hint-1***

The joint posterior distribution $(\mu,\sigma^{2})$, is:  
\[
\begin{cases}
\mu|x_{1:n},\sigma^2 
& \sim 
\text{N}(
\mu_{n},\frac{\sigma^{2}}{\lambda_{n}}
) \\
\sigma^2|x_{1:n} 
& \sim 
\text{IG}(
a_{n},b_{n}
)
\end{cases}
\]

where  
\begin{align*}
\mu_{n}=&
\frac{\lambda_{0}\mu_{n}+n\bar{x}}{\lambda_{0}+n} \\
\lambda_{n}=&
\lambda_{0}+n, \\
a_{n}=&
a_{0}+\frac{n}{2} \\
b_{n}=&
b_{0}+\frac{1}{2}ns^{2}+\frac{1}{2}(\lambda_{0}+n)^{-1}\lambda_{0}n(\mu_{0}-\bar{x})^{2} \\
\end{align*}

Hence the PDF is 
$$
\pi(\mu,\sigma^{2}|x_{1:n}) = 
\text{N}(\mu|\mu_{n},\frac{\sigma^{2}}{\lambda_{n}})
\text{IG}(\sigma^{2}|a_{n},b_{n})
$$

***Hint:*** 

Additional functions for PDFs and CDFs are given   

```{r}
# Inverse Gamma PDF  
invgamma_pdf <- function (x,a,b) {
  return(exp(a*log(b)-lgamma(a)-(a+1)*log(x)-b/x))
}
# Inverse Gamma CDF
invgamma_cdf <- function (x,a,b) {
  return(1.0-pgamma(1.0/x,shape=a,scale=1.0/b) )
}
```


### ... your answer {-}

Below try to address the task by creating R chunks, and running them, and adding comments tot he document.

```{r}
# write your R code


```




# Task  

Produce and plot the MC approximation of the joint PDF of $(\mu,\sigma^2)$, as a 2D histogram, and as a 2D contour plot.

To draw histograms of 2D samples like required by the MC approximation of the PDF you can use the functions `hist2d{gplots}` and  `persp{graphics}`, or `contour{graphics}` as:  

    % known vectors x and y of length 100 having the 2D samples:  
    
    h2d <- hist2d(x,y,nbins=c(20,30),show=FALSE)
    
    persp( h2d$x, h2d$y, h2d$counts, ticktype="detailed", theta=30, phi=30,expand=0.5, shade=0.5, col="cyan", ltheta=-30)
    
    contour( h2d$x, h2d$y, h2d$counts, ticktype="detailed", theta=30, phi=30,expand=0.5, shade=0.5, col="cyan", ltheta=-30)


### ... your answer {-}

Below try to address the task by creating R chunks, and running them, and adding comments tot he document.

```{r}
# write your R code


```

```{r}
library(gplots)
```
 


# Task  

Produce and plot the Exact   marginal PDF   of $\mu$. 

Produce and plotthe MC approximation of the marginal PDF   of $\mu$. 

You can use the functions  `hist {graphics}`  and  `lines {graphics}` or  `plots {graphics}`

***Hint***

The marginal distribution of $\mu$, is:  
  \[
    \mu|x_{1:n}\sim\text{St}_{k}(\mu_{n},\frac{b_{n}}{\lambda_{n}a_{n}},2a_{n})
    \]

where  
\begin{align*}
\mu_{n}=&
  \frac{\lambda_{0}\mu_{n}+n\bar{x}}{\lambda_{0}+n} \\
\lambda_{n}=&
  \lambda_{0}+n, \\
a_{n}=&
  a_{0}+\frac{n}{2} \\
b_{n}=&
  b_{0}+\frac{1}{2}ns^{2}+\frac{1}{2}(\lambda_{0}+n)^{-1}\lambda_{0}n(\mu_{0}-\bar{x})^{2} \\
\end{align*}


***Hint:*** 

Additional PDFs and CDFs are given   

```{r}
# Student T PDF
studentT_pdf <- function(x,m,s,v) {
  return(dt((x-m)/sqrt(s),df=v)/sqrt(s))
}
# Student T CDF
studentT_cdf <- function(x,m,s,v) {
  return(pt((x-m)/sqrt(s),df=v))
}
```

### ... your answer {-}

Below try to address the task by creating R chunks, and running them, and adding comments tot he document.

```{r}
# write your R code


```


# Task  

Produce and plot the Exact   marginal PDF of $\sigma^2$. 

Produce and plot  the MC approximation of the marginal PDF   of $\sigma^2$. 


You can use the functions  `hist {graphics}`  and  `lines {graphics}` or  `plot {graphics}`

***Hint***

The marginal distributions of $\sigma^{2}$, as:
  \[
    \sigma^2|x_{1:n} 
    \sim 
    \text{IG}(\sigma^{2}|a_{n},b_{n})
    \]

where  
\begin{align*}
\mu_{n}=&
  \frac{\lambda_{0}\mu_{n}+n\bar{x}}{\lambda_{0}+n} \\
\lambda_{n}=&
  \lambda_{0}+n, \\
a_{n}=&
  a_{0}+\frac{n}{2} \\
b_{n}=&
  b_{0}+\frac{1}{2}ns^{2}+\frac{1}{2}(\lambda_{0}+n)^{-1}\lambda_{0}n(\mu_{0}-\bar{x})^{2} \\
\end{align*}


***Hint:*** 

Additional PDFs and CDFs are given   

```{r}
# Inverse Gamma PDF  
invgamma_pdf <- function (x,a,b) {
  return(exp(a*log(b)-lgamma(a)-(a+1)*log(x)-b/x))
}
# Inverse Gamma CDF
invgamma_cdf <- function (x,a,b) {
  return(1.0-pgamma(1.0/x,shape=a,scale=1.0/b) )
}
```

### ... your answer {-}

Below try to address the task by creating R chunks, and running them, and adding comments tot he document.

```{r}
# write your R code


```



# Task 

+ Compute the MC approximate of the posterior mean of $\mu$, and $\sigma^2$:  
$$
\text{E}_\pi (\mu|y_{1:n})
\approx
\frac{1}{N}\sum_{j=1}^{N}\mu^{(j)}
$$  
and   
$$
\text{E}_\pi (\sigma^2|y_{1:n})
\approx
\frac{1}{N}\sum_{j=1}^{N} \left(\sigma^{(j)}\right)^2
$$  

+ Compute their exact values which are   
$$\text{E}_\pi (\mu|y_{1:n}) = \mu_n$$  
and  
$$\text{E}_\pi (\sigma^2|y_{1:n}) = \frac{b_n}{a_n-1}$$  

### ... your answer {-}

Below try to address the task by creating R chunks, and running them, and adding comments tot he document.

```{r}
# write your R code


```


# Task 

+ Compute the MC approximate of the posterior prabability that the mean  fuel efficiency (in mpg)  of car is greater or equal to  $22.5$  mpg.  
\begin{align}
\text{Pr}_\pi (\mu\ge 22.5|y_{1:n}) 
&=
1-\text{Pr}_\pi (\mu< 22.5|y_{1:n}) \\
&=
1-\text{E}_\pi (\text{1}(\mu< 22.5)|y_{1:n}) \\
&\approx
1-\frac{1}{N}\sum_{j=1}^{N}\text{1}(\mu^{(j)} < 22.5) 
\end{align}   

+ Compute the exact value which is
\begin{align}
\text{Pr}_\pi (\mu\ge 22.5|y_{1:n}) =& 1-\text{Pr}_{\text{St}_{k}(\mu_{n},\frac{b_{n}}{\lambda_{n}a_{n}},2a_{n})}(\mu\le 22.5|y_{1:n}) \\
=&
1-\int_{-\infty}^{22.5}\text{St}_{k}(\mu|\mu_{n},\frac{b_{n}}{\lambda_{n}a_{n}},2a_{n})\text{d}\mu 
\end{align}   

***Hint***

Functions for the PDFs and CDF are given

```{r}
# Student T PDF
studentT_pdf <- function(x,m,s,v) {
  return(dt((x-m)/sqrt(s),df=v)/sqrt(s))
}
# Student T CDF
studentT_cdf <- function(x,m,s,v) {
  return(pt((x-m)/sqrt(s),df=v))
}
```

### ... your answer {-}

Below try to address the task by creating R chunks, and running them, and adding comments tot he document.

```{r}
# write your R code


```


---

# Task

Compute the exact $95\%$ equal tail posterior credible interval of $\mu$.

***Hint***

The marginal distribution of $\mu$, is:  
  \[
    \mu|x_{1:n}\sim\text{St}_{k}(\mu_{n},\frac{b_{n}}{\lambda_{n}a_{n}},2a_{n})
    \]

where  
\begin{align*}
\mu_{n}=&
  \frac{\lambda_{0}\mu_{n}+n\bar{x}}{\lambda_{0}+n} \\
\lambda_{n}=&
  \lambda_{0}+n, \\
a_{n}=&
  a_{0}+\frac{n}{2} \\
b_{n}=&
  b_{0}+\frac{1}{2}ns^{2}+\frac{1}{2}(\lambda_{0}+n)^{-1}\lambda_{0}n(\mu_{0}-\bar{x})^{2} \\
\end{align*}

Compute the MC approximation of the $95\%$ equal tail posterior credible interval of $\mu$.

***Hint:***

Additional PDFs and CDFs are given   

```{r}
# Student T INVERSE CDF
studentT_inv <- function(prob,m,s,v) {
  q = qt( prob, df=v )
  return( m+sqrt(s*v/(v-2))*q )
}
```

### ... your answer {-}

Below try to address the task by creating R chunks, and running them, and adding comments tot he document.

```{r}
# write your R code


```


---


# Task

Compute the exact $95\%$ equal tail posterior credible interval of $\mu$.

***Hint***

The marginal distribution of $\mu$, is:  
  \[
    \mu|x_{1:n}\sim\text{St}_{k}(\mu_{n},\frac{b_{n}}{\lambda_{n}a_{n}},2a_{n})
    \]

where  
\begin{align*}
\mu_{n}=&
  \frac{\lambda_{0}\mu_{n}+n\bar{x}}{\lambda_{0}+n} \\
\lambda_{n}=&
  \lambda_{0}+n, \\
a_{n}=&
  a_{0}+\frac{n}{2} \\
b_{n}=&
  b_{0}+\frac{1}{2}ns^{2}+\frac{1}{2}(\lambda_{0}+n)^{-1}\lambda_{0}n(\mu_{0}-\bar{x})^{2} \\
\end{align*}

Compute the MC approximation of the $95\%$ equal tail posterior credible interval of $\mu$.

***Hint:***

Additional PDFs and CDFs are given   

```{r}
# Student T INVERSE CDF
invgamma_inv <- function (prob, a, b){
    return(qgamma(1 - prob, a, b)^(-1))
}
```

### ... your answer {-}

Below try to address the task by creating R chunks, and running them, and adding comments tot he document.

```{r}
# write your R code


```


---

# Task


+ Compute and plot the MC approximate of the predictive PDF of the next outcome $y_{n+1}$
\begin{align*}
f(y_{n+1}|y_{1:n}) 
=& \int f(y_{n+1}|\mu,\sigma^2) \pi(\mu,\sigma^2|y_{1:n}) \text{d}\mu\text{d}\sigma^2 \\
=& \text{E}_\pi (f(y_{n+1}|\mu,\sigma^2)|y_{1:n}) \\
=& \text{E}_\pi (\text{N}(y_{n+1}|\mu,\sigma^2)|y_{1:n}) \\
\approx &
\frac{1}{N}\sum_{j=1}^{N} \text{N}\left(y_{n+1}|\mu^{(j)},\left(\sigma^{(j)}\right)^2\right)
\end{align*} 
for $y_{n+1}\in(5,40)$ where $f(y_{n+1}|\mu,\sigma^2)$ is a Normal PDF with mean $\mu$ and variance $\sigma^2$.

+ Compute and plot the exact predictive PDF of the next outcome $y_{n+1}$, which is the PDF of 
\[
y_{n+1}|y_{1:n}\sim\text{St}(\mu_n,\frac{\lambda_n b_n}{(\lambda_n+1)a_n},2a_n)
\] 
where
\begin{align*}
\mu_{n}=&
\frac{\lambda_{0}\mu_{n}+n\bar{x}}{\lambda_{0}+n} \\
\lambda_{n}=&
\lambda_{0}+n, \\
a_{n}=&
a_{0}+\frac{n}{2} \\
b_{n}=&
b_{0}+\frac{1}{2}ns^{2}+\frac{1}{2}(\lambda_{0}+n)^{-1}\lambda_{0}n(\mu_{0}-\bar{x})^{2} \\
\end{align*}
 for $y_{n+1}\in(5,40)$.

***Hint***

Additional PDF and CDF functions are given

```{r}
# Student T PDF
studentT_pdf <- function(x,m,s,v) {
  return(dt((x-m)/sqrt(s),df=v)/sqrt(s))
}
# Student T CDF
studentT_cdf <- function(x,m,s,v) {
  return(pt((x-m)/sqrt(s),df=v))
}
```

### ... your answer {-}

Below try to address the task by creating R chunks, and running them, and adding comments tot he document.

```{r}
# write your R code


```



---



# Task

+ Compute   the MC approximate of the predictive expected value of the next outcome $y_{n+1}$
\begin{align*}
\text{E}_f(y_{n+1}|y_{1:n})
=& \int y_{n+1} f(y_{n+1}|y_{1:n}) \text{d} y_{n+1} \\
=& \int y_{n+1} \left( \int f(y_{n+1}|\mu,\sigma^2) \pi(\mu,\sigma^2|y_{1:n}) \text{d}\mu\text{d}\sigma^2 \right) \text{d} y_{n+1} \\
=& \int  \left( \int y_{n+1} f(y_{n+1}|\mu,\sigma^2) \text{d} y_{n+1} \right)  \pi(\mu,\sigma^2|y_{1:n})  \text{d}\mu\text{d}\sigma^2  \\
=& \text{E}_\pi ( \text{E}_f (y_{n+1}|\mu,\sigma^2)  |y_{1:n}) \\
=& \text{E}_\pi (\mu |y_{1:n}) \\
\approx& \frac{1}{N} \sum_{i=1}^{N}  \mu^{(i)}
\end{align*}

+ Compute   the exact  predictive expected value of the next outcome $y_{n+1}$ which is  
\begin{align*}
\text{E}_f(y_{n+1}|y_{1:n})
=& \text{E}_\pi ( \text{E}_f (y_{n+1}|\mu,\sigma^2)  |y_{1:n}) \\
=& \text{E}_\pi (\mu |y_{1:n}) \\
=& \mu_n
\end{align*}
 

### ... your answer {-}

Below try to address the task by creating R chunks, and running them, and adding comments tot he document.

```{r}
# write your R code


```



# Task

+ Compute   the MC approximate of the predictive variance of the next outcome $y_{n+1}$
\begin{align*}
\text{Var}_f(y_{n+1}|y_{1:n})
&=
\text{E}_{\pi} \left( \text{Var}_{f} \left( y_{n+1}|\mu,\sigma^{2}\right) \right)
+\text{Var}_{\pi} \left( \text{E}_{f} \left( y_{n+1}|\mu,\sigma^{2}\right) \right) \\
&=
\text{E}_{\pi} \left( \sigma^{2}|y_{1:n} \right)
+\text{Var}_{\pi} \left(\mu |y_{1:n} \right) \\
&\approx
\frac{1}{N}\sum_{j=1}^{N} \left( \sigma^{(j)} \right)^{2}
+\frac{1}{N}\sum_{j=1}^{N} \left( \left( \mu^{(j)} \right)^{2}\right)
-\left( \frac{1}{N}\sum_{j=1}^{N} \mu^{(j)} \right)^{2} 
\end{align*}

+ Compute   the exact  predictive expected value of the next outcome $y_{n+1}$ which is  
\begin{align*}
\text{Var}_f(y_{n+1}|y_{1:n})
&=
\frac{\lambda_n b_n}{(\lambda_n+1)(a_n-1)}
\end{align*} 
where
\begin{align*}
\mu_{n}=&
\frac{\lambda_{0}\mu_{n}+n\bar{x}}{\lambda_{0}+n} \\
\lambda_{n}=&
\lambda_{0}+n, \\
a_{n}=&
a_{0}+\frac{n}{2} \\
b_{n}=&
b_{0}+\frac{1}{2}ns^{2}+\frac{1}{2}(\lambda_{0}+n)^{-1}\lambda_{0}n(\mu_{0}-\bar{x})^{2} \\
\end{align*}
because 
\[
y_{n+1}|y_{1:n}\sim\text{St}(\mu_n,\frac{\lambda_n b_n}{(\lambda_n+1)a_n},2a_n)
\] 


### ... your answer {-}

Below try to address the task by creating R chunks, and running them, and adding comments tot he document.

```{r}
# write your R code


```



