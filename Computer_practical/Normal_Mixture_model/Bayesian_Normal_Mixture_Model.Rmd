<!-- -------------------------------------------------------------------------------- -->

<!-- Copyright 2019 Georgios Karagiannis -->

<!-- georgios.karagiannis@durham.ac.uk -->
<!-- Assistant Professor -->
<!-- Department of Mathematical Sciences, Durham University, Durham,  UK  -->

<!-- This file is part of Bayesian_Statistics (MATH3341/4031 Bayesian Statistics III/IV) -->
<!-- which is the material of the course (MATH3341/4031 Bayesian Statistics III/IV) -->
<!-- taught by Georgios P. Katagiannis in the Department of Mathematical Sciences   -->
<!-- in the University of Durham  in Michaelmas term in 2019 -->

<!-- Bayesian_Statistics is free software: you can redistribute it and/or modify -->
<!-- it under the terms of the GNU General Public License as published by -->
<!-- the Free Software Foundation version 3 of the License. -->

<!-- Bayesian_Statistics is distributed in the hope that it will be useful, -->
<!-- but WITHOUT ANY WARRANTY; without even the implied warranty of -->
<!-- MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the -->
<!-- GNU General Public License for more details. -->

<!-- You should have received a copy of the GNU General Public License -->
<!-- along with Bayesian_Statistics  If not, see <http://www.gnu.org/licenses/>. -->

<!-- -------------------------------------------------------------------------------- -->



---
title: "Bayesian Normal mixture model"
subtitle: "Case study: enzyme data-set"
author: "Georgios P. Karagiannis @ MATH3341/4031 Bayesian statistics III/IV"
output:
  html_notebook: 
    number_sections: true
  word_document: default
  html_document:
    df_print: paged
    number_sections: true
  pdf_document: default
header-includes: 
  - \usepackage{tikz}
  - \usepackage{pgfplots}
---
[Back to README](https://github.com/georgios-stats/Bayesian_Statistics/tree/master/ComputerPracticals#aim)

```{r, results="hide"}
rm(list=ls())
```


---

***Aim***

Students will become able to:  

+ produce Monte Carlo approximations of posterior quantities required for Bayesian analysis with the RJAGS R package  

+ implement Bayesian posterior analysis in R with RJAGS package  

Students are not required to learn by heart any of the concepts discussed

---

***Reading material***

*The material about RJAGS package is not examinable material, but it is provided for the interested student. It contains references that students can follow if they want to further explore the concepts introdced.*

+ Lecture notes:  
    + the examples and exercises related to the Bernoulli model with conjugate prior  

+ References for *rjags*:  
    + [JAGS homepage](http://mcmc-jags.sourceforge.net)  
    + [JAGS R CRAN Repository](https://cran.r-project.org/web/packages/rjags/index.html)  
    + [JAGS Reference Manual](https://cran.r-project.org/web/packages/rjags/rjags.pdf)  
    + [JAGS user manual](https://sourceforge.net/projects/mcmc-jags/files/Manuals/4.x/jags_user_manual.pdf/download) 

+ Reference for *R*:  
    + [Cheat sheet with basic commands](https://www.rstudio.com/wp-content/uploads/2016/10/r-cheat-sheet-3.pdf)   

+ Reference of *rmarkdown* (optional):  
    + [R Markdown cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf)  
    + [R Markdown Reference Guide](http://442r58kc8ke1y38f62ssb208-wpengine.netdna-ssl.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf)  
    + [knitr options](https://yihui.name/knitr/options)

+ Reference for *Latex* (optional):  
    + [Latex Cheat Sheet](https://wch.github.io/latexsheet/latexsheet-a4.pdf)  

---

***New software***   

+ R package `rjags` functions:    
    + `jags.model{rjags}`  
    + `jags.samples{rjags}` 
    + `update{rjags}` 

---


```{r, results="hide"}
# Load R package for printing
library(knitr)
library(kableExtra)
```


# Application: Bayesian mixture model    {-}

Consider the following application where our concern is the distribution of enzymatic activity in the blood, for an enzyme involved in the metabolism of carcinogenic substances, among a group of n=245 unrelated individuals; aka cluster analysis.

As for the sampling model, we can assume that the i-th observation $y_{i}$ is randomly drawn from the $j$-th group which has proportion $\varpi_{j}$ in the population and which is distributed according to the sampling distribution $$y_{i}|\theta_{j}\sim f_{j}(y_{i}|\theta_{j}).$$

For simplicity, let's assume that all groups are Normally distributed but with different parameter values $\{\theta_{j}\}$; hence j-th group is $y_{i}|\mu_{j},\sigma_{j}^{2}\sim\text{N}(y_{i}|\mu_{j},\sigma_{j}^{2})$ with  $\theta_{j}=(\mu_{j},\sigma_{j}^{2})$.

```{r}
# load the data
mydata <- scan('https://raw.githubusercontent.com/georgios-stats/Bayesian_Statistics/master/ComputerPracticals/Normal_Mixture_model/enz.dat')
y_obs <- mydata
```

We can apply a histogram to represent the distribution of the observations. 

```{r}
# plot observations
fntsz <- 1.5
hist(y_obs,
     freq = FALSE,
     breaks = 12,
     main = ' ',
     xlab = 'values', 
     ylab = 'pdf estimation', 
     cex.lab=fntsz, 
     cex.axis=fntsz, 
     cex.main=fntsz, 
     cex.sub=fntsz)
```


---

# Model specification & posterior sampling  {-}

It is natural to regard the group label $z_{i}$ for the $i$th observation
as a latent allocation variable: then $z_{i}$ is supposed to be distributed
as $z_{i}\sim f(z_{i})=\varpi_{z_{i}}$ for $z_{i}\in\{1,...,k\}$,
and $y_{i}$ is supposed to be distributed as $y_{i}|z_{i},\theta_{z_{i}}\sim f_{z_{i}}(y_{i}|z_{i},\theta_{z_{i}}):=\text{N}(y_{i}|\mu_{z_{i}},\sigma_{z_{i}}^{2})$,
for $i=1,...,n$; i.e.
$$
\begin{cases}
y_{i}|z_{i},\mu_{z_{i}},\sigma_{z_{i}}^{2} & \sim f_{j}(y_{i}|\mu_{z_{i}},\sigma_{z_{i}}^{2})\\
z_{i} & \sim f(z_{i})
\end{cases}\implies\begin{cases}
y_{i}|z_{i},\mu_{z_{i}},\sigma_{z_{i}}^{2} & \sim\text{N}(y_{i}|\mu_{z_{i}},\sigma_{z_{i}}^{2})\\
z_{i} & \sim f(z_{i}):=\varpi_{z_{i}}
\end{cases} 
$$ 

To complete the Bayesian model, we specify priors on the unknown quantities:
Given there are $k$ groups, $\varpi_{1:k}\sim\text{Di}(\delta)$
for the group proportions, $\mu_{j}\sim\text{N}(\xi_{j},\sigma_{j}^{2}/\kappa)$
for the mean, and $\sigma_{j}^{2}\sim\text{Ga}(\alpha,\beta)$ for
the variances. Assume we wish a more spread prior for $\sigma_{j}^{2}$
(for some reason), and hence we specify a hyper-prior on $\beta$
as $\beta\sim\text{Ga}(g,h)$. As the number of the groups is unknown,
we assign prior $k\sim\pi(k)\in\text{U}_{\text{discr}}(1,k_{\max})$.

$$
\begin{cases}
\quad\quad y_{i}|z_{i},\varpi,\mu,\sigma^{2} & \sim f_{z_{i}}(y_{i}|\varpi_{z_{i}},\mu_{z_{i}},\sigma_{z_{i}}^{2})   \quad\quad\text{for}\;i=1,...,n\\
\quad\quad z_{i} & \sim f(z_{i}):=\varpi_{z_{i}} \quad\quad\quad\quad\text{for}\;i=1,...,n\\
\quad\quad\varpi_{1:k} & \sim\text{Di}(\delta_{1:k})\\
\\
\quad\quad\mu_{j}|\sigma_{j}^{2} & \sim\text{N}(\xi,\sigma_{j}^{2}/\kappa) \quad\quad\quad\quad\text{for}\;j=1,...,k\\
\quad\quad\sigma_{j}^{2}|\beta & \sim\text{Ga}(\alpha,\beta) \quad\quad\quad\quad\quad\text{for}\;j=1,...,k \\
 \quad\quad \beta & \sim\text{Ga}(g,h)
\end{cases}
$$


The joint distribution has pdf 
\begin{align*}
p(y_{1:n},z_{1:n},\varpi_{1:k},\mu_{1:k},\sigma_{1:k}^{2}) & =\overset{f(y_{1:n}|z_{1:n},\mu_{1:k},\sigma_{1:k}^{2})}{\overbrace{\prod_{i=1}^{n}\text{N}(y_{i}|\mu_{z_{i}},\sigma_{z_{i}}^{2})}}\overset{f(z_{1:n})}{\overbrace{\prod_{i=1}^{n}\varpi_{z_{i}}}}\overset{\pi(\mu_{1:k}|\sigma_{1:k}^{2})}{\overbrace{\prod_{j=1}^{k}\text{N}(\mu_{k}|\xi_{k},\sigma_{k}^{2}/\kappa)}}\overset{\pi(\sigma_{1:k}^{2}|\beta)}{\overbrace{\prod_{j=1}^{k}\text{Ga}(\sigma_{k}^{2}|\alpha,\beta)}}\overset{\pi(\beta)}{\overbrace{\text{Ga}(\beta|g,h)}}
\end{align*}

The posterior $\pi(k,\varpi,\mu,\sigma^{2},z|y)$ can be computed
with the Bayesian theorem, and factorized as 
\begin{align}
\pi(\varpi,\mu,\sigma^{2},\beta,z|y) & \quad=\frac{p(y,z,\varpi,\mu,\sigma^{2},\beta)}{\int p(y,z,\varpi,\mu,\sigma^{2},\beta)\text{d}(z,\varpi,\mu,\sigma^{2},\beta)}
\end{align}
 where to infer the proportions
in each group from $\pi(\varpi_{1:k}|y)$, the moments of each group
from $\pi(\mu_{1:k},\sigma_{1:k}^{2}|y)$, and the allocation of
each observation to each group with $\pi(z|y,\varpi,\mu,\sigma^{2}).$


As the required integrals are intractable, we can resolve to numerical methods, etc... Monte Carlo e.g. via JAGS....

# JAGS {-}


***step 1***

Load the library

```{r, results="hide"}
# Load rjags
library("rjags") 
```

***step 2***

Create an input script, for rjags, containing the Bayesian hierarchical model

```{r}
# Input parameters  :  
# output parameters :  
jags_model <- "
    model{
    
      for (i in 1:n) {
      
        y[i] ~ dnorm(mu[zeta[i]], tau[zeta[i]])
        
        zeta[i] ~ dcat(varpi[])
      }
  
      for (i in 1:k) {
      
        mu[i] ~ dnorm(xi,kappa)
      
        tau[i] ~ dgamma(alpha,beta)
      
        sigma[i] <- 1/sqrt(tau[i])
      }
      
      beta ~ dgamma(g,h)
      
      varpi ~ ddirich(delta)
    }
"
```

***step 3***

Create an input list, for jags, containing the data and fixed  parameters of the model 

```{r, results="hide"}
# set observations
y_obs <- y_obs 
n_obs <- length( y_obs )
# set priors
k_fix <- 3
delta_fix <- rep(1,k_fix)
xi_fix <- 0.5*(max(y_obs)+min(y_obs))
kappa_fix <- 1.0 /(max(y_obs)-min(y_obs))^2
alpha_fix <- 2.0
h_fix <- 10.0 /(max(y_obs)-min(y_obs))^2
g_fix <- 0.2
# set the list for inputing in the function
data.bayes <- list(y = y_obs, 
                   n = n_obs, 
                   k = k_fix,
                   delta = delta_fix,
                   xi = xi_fix,
                   kappa = kappa_fix, 
                   alpha = alpha_fix,
                   h = h_fix,
                   g = g_fix
                   )
# set the JAGS model
model.smpl <- jags.model( file = textConnection(jags_model),
                          data = data.bayes, 
                          n.chains = 1)
# tune the JAGS MCCM sampler
adapt(object = model.smpl, 
      n.iter = 10^4 )
# Generate a posterior sample of size $N=10000$.
Nsample = 10^4              # the size of the sample we ll gona get
n.thin = 10^1               # the thining (improving) the sample quality
n.iter = Nsample * n.thin   # the number of the total iterations performed
output = jags.samples( model = model.smpl,                          # the model
                       variable.names = c('varpi','mu','sigma','zeta'),    # names of variables to be sampled
                       n.iter = n.iter,                             # size of sample
                       thin = n.thin
                       ) 
# save the sample
save.image(file="BayesianNormalMixtureModel.RData") 
# Check the names of the variables sampled 
names(output)
```


# Prediction {-}

### Fitting/prediction {-}

We estimate the pdf of the distribution of enzyme. 

Recall that the compound distribution of 
\[
\begin{cases}
y_{i}|z_{i},\mu_{z_{i}},\sigma_{z_{i}}^{2} & \sim f(y_{i}|\mu_{z_{i}},\sigma_{z_{i}}^{2})\\
z_{i} & \sim f(z_{i})
\end{cases}\implies\begin{cases}
y_{i}|z_{i},\mu_{z_{i}},\sigma_{z_{i}}^{2} & \sim\text{N}(y_{i}|\mu_{z_{i}},\sigma_{z_{i}}^{2})\\
z_{i} & \sim f(z_{i}):=\varpi_{z_{i}}
\end{cases}
\]
 when $z$ is discrete is 
\begin{align*}
\text{E}_{\Pi(z|y,\varpi,\mu,\sigma^{2})}\left(f\left(y^{*}|\mu_{z},\sigma_{z}^{2}\right)|y\right) & =\sum_{j=1}^{k}\varpi_{j}f\left(y^{*}|\mu_{j},\sigma_{j}^{2}\right)\\
 & =\sum_{j=1}^{k}\varpi_{j}\text{N}\left(y^{*}|\mu_{j},\sigma_{j}^{2}\right)
\end{align*}
 Hence   the predictor under the quadratic loss is the posterior mean:
\begin{align*}
\text{E}_{\Pi}\left(f\left(y^{*}|\mu_{z},\sigma_{z}^{2}\right)|y\right) & =\text{E}_{\Pi\left(\varpi,\mu,\sigma^{2}|y\right)}\left(\text{E}_{\Pi(z|y,\varpi,\mu,\sigma^{2})}\left(f\left(y^{*}|\mu_{z},\sigma_{z}^{2}\right)|y\right)\right)\\
 & =\text{E}_{\Pi\left(\varpi,\mu,\sigma^{2}|y\right)}\left(\sum_{j=1}^{k}\varpi_{j}f\left(y^{*}|\mu_{j},\sigma_{j}^{2}\right)|y\right)\\
 & \approx\frac{1}{T}\sum_{t=1}^{T}\left[\sum_{j=1}^{k}\varpi_{j}^{(t)}f\left(y^{*}|\mu_{j}^{(t)},\left(\sigma_{j}^{(t)}\right)^{2}\right)\right]\\
 & \approx\frac{1}{T}\sum_{t=1}^{T}\left[\sum_{j=1}^{k}\varpi_{j}^{(t)}\text{N}\left(y^{*}|\mu_{j}^{(t)},\left(\sigma_{j}^{(t)}\right)^{2}\right)\right]
\end{align*}

The credible intervals at each $y_{new}$ can be approximated by the quantiles of the MCMC sample (as in the computer practical).  

Now operationally:

Copy the sample of each variable in a vector with a more friendly name...

```{r}
# just store them as vectors
varpi_vec <-output$varpi[,,1]
mu_vec <-output$mu[,,1]
sigma_vec <-output$sigma[,,1]
zeta_vec <- output$zeta[,,1]
for (t in 1:Nsample) {
  ind <- sort(mu_vec[,t],index.return=TRUE)$ix
  varpi_vec[,t] <- varpi_vec[ind,t]
  mu_vec[,t] <- mu_vec[ind,t]
  sigma_vec[,t] <- sigma_vec[ind,t]
}
```

Define the required functions; i.e. $f\left(y^{*}|\varpi_{1:k},\mu_{1:k},\sigma_{1:k}^{2}\right)=\sum_{j=1}^{k}\varpi_{j}f\left(y^{*}|\mu_{j},\sigma_{j}^{2}\right)$

```{r}
# pdf of sampling distribution
fmix <- function(y, varpi, mu, sigma) {
  f <- sum( varpi*dnorm(y, mean = mu, sd = sigma) ) ;
  return ( f  )
}
```

Compute the Monte carlo proxy of the estimate 

```{r}
# set the horizontal axis values
ynew_min = 0.0
ynew_max = 3.2
ynew_size = 100
ynew = seq(from = ynew_min, to = ynew_max, length.out = ynew_size) 
# compute the estimates at ynew
fest_mean = rep(NaN, times = ynew_size)
fest_low = rep(NaN, times = ynew_size)
fest_upper = rep(NaN, times = ynew_size)
vec <- rep(NaN, times = Nsample)
for (i in 1: length(ynew)) {
  for (t in 1:Nsample) {
    vec[t] <- fmix(ynew[i], varpi_vec[t], mu_vec[t], sigma_vec[t])
  }
  fest_low[i] <-  quantile(vec, probs = 0.025)
  fest_upper[i] <-  quantile(vec, probs = 0.975)
  fest_mean[i] <- mean(vec)
}
# print the prediction and the predictive intervals
fntsz <- 1.5
plot(ynew,
     fest_mean,
     main = "predictive distribution",
     xlab = "enzyme",
     ylab = "pdf",
     type = 'l',
     col = 'black',
     cex.lab=fntsz, 
     cex.axis=fntsz, 
     cex.main=fntsz, 
     cex.sub=fntsz
     )
lines (ynew,
     fest_low,
     col = 'blue'
     )
lines (ynew,
     fest_upper,
     col = 'blue'
     )
```


### Classification of an observation {-}


We can estimate to which class the $i$-th observation belongs by computing the posterior probability $\pi(z_i=j|y)$ for all $j=1,...,k$.  

It is for $j=1,...,k$

\begin{align*}
\pi\left(z_{i}=j|y\right) & =\text{E}_{\Pi}\left(\text{1}\left(z_{i}=j\right)|y\right)\\
 & \approx\frac{1}{T}\sum_{t=1}^{T}\text{1}\left(z_{i}^{(t)}=j\right)
\end{align*}

Here we will do the demonstration just for the first $4$ observations of the sample: 

```{r}
pi_i <-rep(NaN, times = k_fix)
par(mfrow=c(2,2))
for (i in 1:4) {
  for (j in 1:k_fix) {
    pi_i[j] <- mean(zeta_vec[i,]==j)
  }
  fntsz < 1.5
  barplot (pi_i,
           names.arg= c(1:k_fix),
           main = paste(as.character(i), '-th obs y=', as.character(y_obs[i]) ),
           cex.lab=fntsz, 
           cex.axis=fntsz, 
           cex.main=fntsz, 
           cex.sub=fntsz)
}

```



### Classification of a new observation {-}


\begin{align*}
\pi\left(z_{i}=j|y\right) & =\text{E}_{\Pi}\left(\text{1}\left(z_{i}=j\right)|y\right)\\
 & \approx\frac{1}{T}\sum_{t=1}^{T}\text{1}\left(z_{i}^{(t)}=j\right)
\end{align*}
I can compute the classification probability of a new measurement
$y^{*}$ as the posterior of its group label $z^{*}$, namely, $\pi\left(z^{*}=j|y\right)$.

Recall that 
\[
\begin{cases}
y^{*}|z^{*},\mu_{z^{*}},\sigma_{z^{*}}^{2} & \sim f(y^{*}|\mu_{z^{*}},\sigma_{z^{*}}^{2})\\
z^{*} & \sim f(z^{*})
\end{cases}\implies f(z^{*}|y^{*},\mu_{z^{*}},\sigma_{z^{*}}^{2})=\frac{f(y^{*},z^{*}|\mu_{z^{*}},\sigma_{z^{*}}^{2})}{\sum_{z}f(y^{*},z^{*}|\mu_{z^{*}},\sigma_{z^{*}}^{2})}=\frac{f(y^{*},z^{*}|\mu_{z^{*}},\sigma_{z^{*}}^{2})}{f(y^{*}|\mu_{z^{*}},\sigma_{z^{*}}^{2})}
\]
 Then 
\begin{align*}
\pi\left(z^{*}=j|y\right) & =\text{E}_{\Pi}\left(f(z^{*}=j|\mu_{z},\sigma_{z}^{2})|y\right)\:=\text{E}_{\Pi}\left(\frac{f(y^{*},z^{*}=j|\mu_{z},\sigma_{z}^{2})}{f(y^{*}|\mu_{z},\sigma_{z}^{2})}|y\right)\\
 & =\text{E}_{\Pi\left(\varpi,\mu,\sigma^{2}|y\right)}\left(\text{E}_{\Pi(z|y,\varpi,\mu,\sigma^{2})}\left(\frac{f(y^{*},z^{*}=j|\mu_{z},\sigma_{z}^{2})}{f(y^{*}|\mu_{z},\sigma_{z}^{2})}|y\right)\right)\\
 & =\text{E}_{\Pi\left(\varpi,\mu,\sigma^{2}|y\right)}\left(\frac{\varpi_{j}f\left(y^{*}|\mu_{j},\sigma_{j}^{2}\right)}{\sum_{j'=1}^{k}\varpi_{j'}f\left(y^{*}|\mu_{j'},\sigma_{j'}^{2}\right)}|y\right)\\
 & =\frac{1}{T}\sum_{t=1}^{T}\left[\frac{\varpi_{j}^{(t)}f\left(y^{*}|\mu_{j}^{(t)},\left(\sigma_{j}^{(t)}\right)^{2}\right)}{\sum_{j'=1}^{k}\varpi_{j'}^{(t)}f\left(y^{*}|\mu_{j'}^{(t)},\left(\sigma_{j'}^{(t)}\right)^{2}\right)}\right]
\end{align*}






Suppose we wish to see to which group we classify a new observation $y^{*}=1.1$. I nees to compute the posterior probability $\pi(z^{*}|y)$ for the $z^{*}$ corresponding to $y^{*}=1.1$.  

```{r}
# pdf of sampling distribution
fclass <- function(k , y, varpi, mu, sigma) {
  f <- rep(NaN, rep=k)
  for (j in 1:k)
    f[j] <- varpi[j]*dnorm(y, mean = mu[j], sd = sigma[j]) 
  f <- f / sum(f) ;
  return ( f  )
}
# compute the estimates at ynew
ynew = 1.1 
fclass_est <- rep(0.0,k_fix)
for (t in 1:Nsample) {
  fclass_est <- fclass_est + fclass(k_fix, ynew, varpi_vec[,t], mu_vec[,t], sigma_vec[,t])
}
fclass_est <- fclass_est / Nsample
# print the predictive distribution of the group label z* of y* 
fntsz < 1.5
barplot (fclass_est,
           names.arg= c(1:k_fix),
           main = paste('pi(z*|y) of  y* =', as.character(ynew) ),
           cex.lab=fntsz, 
           cex.axis=fntsz, 
           cex.main=fntsz, 
           cex.sub=fntsz)

```


.  

---

