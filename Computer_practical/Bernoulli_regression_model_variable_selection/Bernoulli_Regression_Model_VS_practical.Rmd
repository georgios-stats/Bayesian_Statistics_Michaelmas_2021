---
title: "Bernoulli regression model (Variable Selection)"
subtitle: "Case study: Space shuttle Challenger disaster"
author: "Georgios P. Karagiannis @ MATH3341/4031 Bayesian statistics III/IV (practical implementation)"
header-includes:
   - \usepackage{mathrsfs}
output:
  html_document:
    df_print: paged
    number_sections: true
  word_document: default
  html_notebook: 
    number_sections: true
  pdf_document: default
---

<!-- -------------------------------------------------------------------------------- -->

<!-- Copyright 2019 Georgios Karagiannis -->

<!-- georgios.karagiannis@durham.ac.uk -->
<!-- Assistant Professor -->
<!-- Department of Mathematical Sciences, Durham University, Durham,  UK  -->

<!-- This file is part of Bayesian_Statistics (MATH3341/4031 Bayesian Statistics III/IV) -->
<!-- which is the material of the course (MATH3341/4031 Bayesian Statistics III/IV) -->
<!-- taught by Georgios P. Katagiannis in the Department of Mathematical Sciences   -->
<!-- in the University of Durham  in Michaelmas term in 2019 -->

<!-- Bayesian_Statistics is free software: you can redistribute it and/or modify -->
<!-- it under the terms of the GNU General Public License as published by -->
<!-- the Free Software Foundation version 3 of the License. -->

<!-- Bayesian_Statistics is distributed in the hope that it will be useful, -->
<!-- but WITHOUT ANY WARRANTY; without even the implied warranty of -->
<!-- MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the -->
<!-- GNU General Public License for more details. -->

<!-- You should have received a copy of the GNU General Public License -->
<!-- along with Bayesian_Statistics  If not, see <http://www.gnu.org/licenses/>. -->

<!-- -------------------------------------------------------------------------------- -->


[Back to README](https://github.com/georgios-stats/Bayesian_Statistics_Michaelmas_2021/tree/main/Computer_practical#aim)

```{r, results="hide"}
rm(list=ls())
```


---

***Aim***

Students will become able to:  

+ produce Monte Carlo approximations of posterior quantities required for Bayesian analysis with the RJAGS R package  

+ implement Bayesian posterior analysis in R with RJAGS package  

Students are not required to learn by heart any of the concepts discussed

---

***Reading material***

*The material about RJAGS package is not examinable material, but it is provided for the interested student. It contains references that students can follow if they want to further explore the concepts introdced.*

+ Lecture notes:  
    + the examples and exercises related to the Bernoulli model with conjugate prior  
    
+ Application (optional):
    + [Dalal, S. R., Fowlkes, E. B., & Hoadley, B. (1989). Risk analysis of the space shuttle: Pre-Challenger prediction of failure. Journal of the American Statistical Association, 84(408), 945-957.](https://www.tandfonline.com/doi/abs/10.1080/01621459.1989.10478858)    

+ References for *rjags*:  
    + [JAGS homepage](http://mcmc-jags.sourceforge.net)  
    + [JAGS R CRAN Repository](https://cran.r-project.org/web/packages/rjags/index.html)  
    + [JAGS Reference Manual](https://cran.r-project.org/web/packages/rjags/rjags.pdf)  
    + [JAGS user manual](https://sourceforge.net/projects/mcmc-jags/files/Manuals/4.x/jags_user_manual.pdf/download) 

+ Reference for *R*:  
    + [Cheat sheet with basic commands](https://www.rstudio.com/wp-content/uploads/2016/10/r-cheat-sheet-3.pdf)   

+ Reference of *rmarkdown* (optional):  
    + [R Markdown cheatsheet](https://www.rstudio.com/wp-content/uploads/2016/03/rmarkdown-cheatsheet-2.0.pdf)  
    + [R Markdown Reference Guide](http://442r58kc8ke1y38f62ssb208-wpengine.netdna-ssl.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf)  
    + [knitr options](https://yihui.name/knitr/options)

+ Reference for *Latex* (optional):  
    + [Latex Cheat Sheet](https://wch.github.io/latexsheet/latexsheet-a4.pdf)  

---

***New software***   

+ R package `rjags` functions:    
    + `jags.model{rjags}`  
    + `jags.samples{rjags}` 
    + `update{rjags}` 


---


# Application: Challenger O-ring    {-}

On January 28, 1986, a routine launch was anticipated for the Challenger space shuttle. Seventy-three seconds into the flight, disaster happened: the shuttle broke apart, killing all seven crew members on board. [Here is the video](https://youtu.be/fSTrmJtHLFU?t=99). 

The Rogers Commission report on the space shuttle Challenger accident concluded that the accident was caused by a combustion gas leak through a joint in one of the booster rockets, which was sealed by a device called an O-ring. The Challenger accident was caused by gas leak through the 6 O-ring joints of the shuttle. 

The commission further concluded that 0-rings do not seal properly at low temperatures.

[Dalal, Fowlkes and Hoadley (1989)](https://www.jstor.org/stable/pdf/2290069.pdf)   looked at the number of distressed O-rings (among the 6) for 23 previous shuttle flights, and the data-set is provided below.  In the table below presents data from the 23 preaccident launches of the space shuttle is used to predict 0-ring performance under the Challenger launch conditions and relate it to the catastrophic failure of the shuttle. The the data-set is provided below, where in column *Defective.O.rings*, (1) stands for presence of at least one distressed O-ring, and (0) stands for absence of any distressed O-ring; while the rest columns are self explained. 

```{r, results="hide"}
# Load R package for printing
library(knitr)
library(kableExtra)
```

```{r}
# load the data
#mydata <- read.csv("./challenger_data.csv")
mydata <- read.csv("https://raw.githubusercontent.com/georgios-stats/Bayesian_Statistics_Michaelmas_2021/main/Computer_practical/Bernoulli_regression_model/challenger_data.csv")
# print data 
## (that's a sophisticated command with fancy output, feel free to ignore it)
kable(mydata)%>%
  kable_styling(bootstrap_options = c("striped", "hover"))
```

On the night of January 27, 1986, the night before the space shuttle Challenger accident, there was a three-hour teleconference among people at Morton Thiokol, Marshall Space Flight Center, and Kennedy Space Center. The discussion focused on the forecast of a 31F temperature for launch time the next morning, and the effect of low temperature on 0-ring performance. 

We are interested in finding if any (or both) of the variables *Damage.Incident* and *Temperature* can be characterised as discriminator for the occirance of a defective O-ring. 

To answer the above we perform Bayesian analysis based on the observed data-set on the dates from *04/12/1981* to *01/12/1986*, and the variables *Damage.Incident* and *Temperature*. So ignore the variable *Leak.check.pressure*.

---




# Model specification & posterior sampling 

Let $y_{i}$ denote the presence of a defective O-ring in the $i$th flight ($0$ for absence, and $1$ for presence).

**Regarding the statistical model**, we assume that $y_{i}$ can be modeled as observations generated independently from a Bernoulli distribution with with parameter $p_{i}$. Here, $p_{i}$ denotes the relative frequency of defective O-rings at flight $i$. We drop the assumption of homogeneity in the parameters!!!  

As we are interesting in studing if the outcome variable $y$: 'presence of a defective O-ring' depends on  the input variable $t$: 'temperature', or $s$: 'pressure'. 

Let $t_{i}$ denote the temperature (in F) in the platform before the $i$th flight.  

Let $s_{i}$ denote the Leak check pressure (in PSI).

Here are some possible models of interest:

\begin{align*}
\mathscr{M}^{I}:\ \ 
p(t;\beta_{\mathscr{M}^{I}},\mathscr{M}^{I}) 
=&
\frac{\exp(\beta_{0})}{1+\exp(\beta_{0})} 
\end{align*}

\begin{align*}
\mathscr{M}^{II}:\ \ 
p(t;\beta_{\mathscr{M}^{II}},\mathscr{M}^{II}) 
=&
\frac{\exp(\beta_0+\beta_1 t)}{1+\exp(\beta_0+\beta_1 t)} 
\end{align*}

\begin{align*}
\mathscr{M}^{III}:\ \ 
p(t;\beta_{\mathscr{M}^{III}},\mathscr{M}^{III}) 
=&
\frac{\exp(\beta_0+\beta_2 s)}{1+\exp(\beta_0+\beta_2 s)} 
\end{align*}

\begin{align*}
\mathscr{M}^{IV}:\ \ 
p(t;\beta_{\mathscr{M}^{IV}},\mathscr{M}^{IV}) 
=&
\frac{\exp(\beta_0+\beta_1 t+\beta_2 s)}{1+\exp(\beta_0+\beta_1 t+\beta_2 s)} 
\end{align*}

\begin{align*}
\mathscr{M}^{V}:\ \ 
p(t;\beta_{\mathscr{M}^{V}},\mathscr{M}^{V}) 
=&
\frac{\exp(\beta_0+\beta_1 t+\beta_2 s+\beta_3 ts)}{1+\exp(\beta_0+\beta_1 t+\beta_2 s+\beta_3 ts)}
\end{align*}

In the models above, we consider standardise the input variables as  $$t_{i}\leftarrow\frac{t_{i}-\bar{t}}{\sqrt{s_t^2}}$$ and $$s_{i}\leftarrow\frac{s_{i}-\bar{s}}{\sqrt{s_s^2}}$$, in order to eliminate the unites and have the same uning in bothe sides of the equality. 


**Regarding the prior model**, we assign a Normal prior distribution, with mean hyper-parameter $b_0$ and variance hyper-parameter $B_0$, on the unknown parameter $\beta$ to account for the uncertainty about it. 

Hmmmm... we could use other priors too ... I just picked one ...   

**The Bayesian hierarchical model** under consideration is: 

\[
\begin{cases}
y_{i}|\mathscr{M},\beta_{\mathscr{M}} & \sim\text{Bernoulli}(p(x_{i};\mathscr{M},\beta_{\mathscr{M}})),\quad\text{for, }i=1,...,n\\
\\
p(x_{i};\mathscr{M},\beta_{\mathscr{M}}) & =\frac{\exp(x_{i,\mathscr{M}}^{\top}\beta_{\mathscr{M}})}{1+\exp(x_{i,\mathscr{M}}^{\top}\beta_{\mathscr{M}})};\,\,\text{where}\,\,x_{i,\mathscr{M}}^{\top}\beta_{\mathscr{M}}=\sum_{j\in\mathscr{M}}x_{i,j}\beta_{j}\\
\\
\beta_{j}|j\in\mathscr{M} & \sim\text{N}(\beta_{j}|\mu_{0},\sigma_{0}^{2}),\ \ j=1,...,d\\
\\
\mathscr{M} & =\left\{ j\in\left\{ 1,...,d\right\} ,\,\text{s.t.}\,\gamma_{j}=1\right\} \\
\\
\gamma_{j} & \sim\text{Bernoulli}(\varpi),\ \ j=1,...,d\\
\\
\varpi & \sim\text{Be}(a_{0},b_{0})
\end{cases}
\]
with hyper-parameter values $\mu_0=0.0$, $\sigma_0^2=100.0$, and $\varpi_0=0.5$.   

**Computational trick by augmentation: The augmented Bayesian hierarchical model** 

Notice that the length of the join unknown parameter vector  $\vartheta_{k}=\left(\mathscr{M},\beta_{\mathscr{M}},\gamma_{1:d},\varpi\right)$ changes when $\mathscr{M}$.  

Current versions of JAGS work only when the join unknown parameter vector $\vartheta_{k}$ has a fixed length.  

To make JAGS work on our problem we 'cheat' by augmenting $\vartheta_{k}$ with additional random variables $\beta_{j}|j\notin\mathscr{M}  \sim\text{N}(\beta_{j}|\mu_{0},\sigma_{0}^{2})$.   Precisely, we consider the augmented Bayesian hierarchical model
\[
\begin{cases}
y_{i}|\mathscr{M},\beta_{\mathscr{M}} & \sim\text{Bernoulli}(p(x_{i};\mathscr{M},\beta_{\mathscr{M}})),\quad\text{for, }i=1,...,n\\
\\
p(x_{i};\mathscr{M},\beta_{\mathscr{M}}) & =\frac{\exp(x_{i,\mathscr{M}}^{\top}\beta_{\mathscr{M}})}{1+\exp(x_{i,\mathscr{M}}^{\top}\beta_{\mathscr{M}})};\,\,\text{where}\,\,x_{i,\mathscr{M}}^{\top}\beta_{\mathscr{M}}=\sum_{j\in\mathscr{M}}x_{i,j}\beta_{j}\\
\\
\beta_{j}|j\in\mathscr{M} & \sim\text{N}(\beta_{j}|\mu_{0},\sigma_{0}^{2}),\ \ j=1,...,d\\
\beta_{j}|j\notin\mathscr{M} & \sim\text{N}(\beta_{j}|\mu_{0},\sigma_{0}^{2}),\ \ j=1,...,d\\
\\
\mathscr{M} & =\left\{ j\in\left\{ 1,...,d\right\} ,\,\text{s.t.}\,\gamma_{j}=1\right\} \\
\\
\gamma_{j} & \sim\text{Bernoulli}(\varpi),\ \ j=1,...,d\\
\\
\varpi & \sim\text{Be}(a_{0},b_{0})
\end{cases}
\]
Now we work on $\tilde{\theta}_{k}=\left(\theta_{k},\beta_{\mathscr{M}^{\complement}}\right)=\left(\mathscr{M},\beta_{\mathscr{M}},\beta_{\mathscr{M}^{\complement}},\gamma_{1:d},\varpi\right)$. The augmented hierarchical model admits the hierarchical model as its marginal because
\[
\pi\left(\mathscr{M},\beta_{\mathscr{M}},\gamma_{1:d},\varpi|y\right)=\int\pi\left(\mathscr{M},\beta_{\mathscr{M}},\beta_{\mathscr{M}^{\complement}},\gamma_{1:d},\varpi|y\right)\text{d}\beta_{\mathscr{M}^{\complement}}
\]
What we do:  

1. Run RJAGS against the augmented Bayesian hierarchical model and generate samples of   $\tilde{\theta}_{k}=\left(\mathscr{M},\beta_{\mathscr{M}},\beta_{\mathscr{M}^{\complement}},\gamma_{1:d},\varpi\right)$.  

2. To gen the samples from $\pi\left(\mathscr{M},\beta_{\mathscr{M}},\gamma_{1:d},\varpi|y\right)$, you just need to ignore the generated values of $\beta_{\mathscr{M}^{\complement}}$.   


### ... your answer {-}

Load the library

```{r}
# Load JAGS
library(rjags)
```

Create an input script, for rjags, containing the Bayesian hierarchical model 

```{r}

# init <- list( beta = rep(0,dmax), 
#               ind = rep(0,dmax),
#               pp = 0.5)

hierarhicalmodel="
  model {
    
    # sampling distribution
    
    for (i in 1:n) {
      eta[i] <- inprod(X[i,],beta*ind)
      mean[i] <- exp( eta[i] ) / ( 1 + exp( eta[i] ) )
      y[i] ~ dbern(mean[i])
    }
    
    # within model prior + augmentation
    
    for ( j in 1:dmax ) {
      beta[j] ~ dnorm( 0 , 0.1 )
    }
    
    # marginal model prior
    
    ind[1] <- 1
    for (j in 2:dmax) {

      ind[j] ~ dbern( pp )
    }
    
    # hyper-prior
    
    pp ~ dbeta(1.0,1.0)
  }
"
```


Create an input list, for jags, containing the data and fixed  parameters of the model 




```{r}
#
#
#
```




Create an input list, for jags, containing the data and fixed  parameters of the model 

```{r, results="hide"}
#
#
```

Initialize the sampler with $N_{\text{adapt}}=1000$ iterations.   


```{r, results="hide"}
#
#
```


Generate a posterior sample of size $N=10000$.


```{r, results="hide"}
#
#
```

Check the names of the variables sampled  

```{r}
#
```

Check the dimensions of each of the variables sampled  

```{r}
#
```

Copy the sample of each variable in a vector with a more friendly name...

```{r}
#
```

# Task

Calculate the marginal posterior model probabilities of models $\mathscr{M}^{I}=(1,0,0,0)$, $\mathscr{M}^{II}=(1,1,0,0)$, $\mathscr{M}^{III}=(1,0,1,0)$,  $\mathscr{M}^{IV}=(1,1,1,0)$, $\mathscr{M}^{V}=(1,1,1,1)$, $\mathscr{M}^{VI}=(1,0,0,1)$,  $\mathscr{M}^{VII}=(1,1,0,1)$, and $\mathscr{M}^{VIII}=(1,0,1,1)$  :

$$
\Pi(\mathscr{M}^{I}|y_{1:n})=
\Pi(\mathscr{M}^{I}=(1,0,0,0)|y_{1:n})
$$

$$
\Pi(\mathscr{M}^{II}|y_{1:n})=
\Pi(\mathscr{M}^{II}=(1,1,0,0)|y_{1:n})
$$

$$
\Pi(\mathscr{M}^{III}|y_{1:n})=
\Pi(\mathscr{M}^{III}=(1,0,1,0)|y_{1:n})
$$
$$
\Pi(\mathscr{M}^{IV}|y_{1:n})=
\Pi(\mathscr{M}^{IV}=(1,1,1,0)|y_{1:n})
$$
$$
\Pi(\mathscr{M}^{V}|y_{1:n})=
\Pi(\mathscr{M}^{V}=(1,1,1,1)|y_{1:n})
$$

$$
\Pi(\mathscr{M}^{VI}|y_{1:n})=
\Pi(\mathscr{M}^{VI}=(1,0,0,1)|y_{1:n})
$$
$$
\Pi(\mathscr{M}^{VII}|y_{1:n})=
\Pi(\mathscr{M}^{VII}=(1,1,0,1)|y_{1:n})
$$

$$
\Pi(\mathscr{M}^{VII}|y_{1:n})=
\Pi(\mathscr{M}^{VII}=(1,0,1,1)|y_{1:n})
$$


### ... your answer {-}

It is 
```{r}
# Compute the marginal distributions

# Print the marginal distributions

# plot the probabilities 

```


# Task

Assume that only one of the model in the set $$\mathcal{M}=\{\mathscr{M}^{I}, \mathscr{M}^{II}, \mathscr{M}^{III}, \mathscr{M}^{IV}, \mathscr{M}^{V}\}$$ is of interest. This is because it is 

Calculate the posterior model probabilities of models $\mathscr{M}^{I}=(1,0,0,0)$, $\mathscr{M}^{II}=(1,1,0,0)$, $\mathscr{M}^{III}=(1,0,1,0)$,  $\mathscr{M}^{IV}=(1,1,1,0)$, $\mathscr{M}^{V}=(1,1,1,1)$, given the model collection $\mathcal{M}$.



$$
\Pi(\mathscr{M}^{I}|y_{1:n},\mathcal{M})=
\Pi(\mathscr{M}^{I}=(1,0,0,0)|y_{1:n},\mathcal{M})
$$

$$
\Pi(\mathscr{M}^{II}|y_{1:n},\mathcal{M})=
\Pi(\mathscr{M}^{II}=(1,1,0,0)|y_{1:n},\mathcal{M})
$$

$$
\Pi(\mathscr{M}^{III}|y_{1:n},\mathcal{M})=
\Pi(\mathscr{M}^{III}=(1,0,1,0)|y_{1:n},\mathcal{M})
$$
$$
\Pi(\mathscr{M}^{IV}|y_{1:n},\mathcal{M})=
\Pi(\mathscr{M}^{IV}=(1,1,1,0)|y_{1:n},\mathcal{M})
$$
$$
\Pi(\mathscr{M}^{V}|y_{1:n},\mathcal{M})=
\Pi(\mathscr{M}^{V}=(1,1,1,1)|y_{1:n},\mathcal{M})
$$



Which model is a posteriori the most probabile?


### ... your answer {-}


It is 
```{r}
# Compute the marginal distributions

# Compute the conditional distributions

# Print the conditional distributions

# plot the probabilities 

```

The most model with the highest marginal model posterior probability is  
\begin{align*}
\mathscr{M}^{II}:\ \ 
p(t;\beta_{\mathscr{M}^{II}},\mathscr{M}^{II}) 
=&
\frac{\exp(\beta_0+\beta_1 t)}{1+\exp(\beta_0+\beta_1 t)} 
\end{align*}





